{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 2021/2022 - Challenge \n",
    "\n",
    "<hr>\n",
    "\n",
    "# Identification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "yourNameSurname='' # e.g., yourNameSurname='Mario Rossi'\n",
    "yourMatricolaNumber='' # e.g., yourMatricolaNumber='12345678'\n",
    "yourStudentEMAIL='' # e.g., yourStudentEMAIL='rossim.12345678@studenti.uniroma1.it'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1. Mandatory Rules:\n",
    "- This year the results of the challenges will count 11,2/28 of your final grade (full info about grades <a href='https://twiki.di.uniroma1.it/twiki/view/ApprAuto '>here</a>).\n",
    "- Only one submission is allowed. We will not consider multiple submissions.\n",
    "- Please remember your solution must be <b>\"YOUR SOLUTION\"</b>, hence you are requested to deliver your individual answers/arguments/opinions/critics.\n",
    "- Mail your solution (with your <b>jupyter notebook</b> and the cleaned dataset) only to stefano.faralli@uniroma1.it <b>deadlines are announced on the ML google group and <a href='https://twiki.di.uniroma1.it/twiki/view/ApprAuto'>here</a> (NO EXCEPTIONS)</b> if you miss to deliver your solution you must wait the next (if any) available deadline. \n",
    "- The subject of your email must be: \"[ML-21-22-Challenge_solution] NAME - SURNAME - MATRICOLA\".\n",
    "- Double check the subject of your email and the attachments.\n",
    "- In case you want to compress the attachment, <b>USE ONLY STANDARD ZIP compression</b> (NO RAR,7Z etc..).\n",
    "- <b>Please sumbit The notebook (with SAVED OUTPUTS) and the cleaned dataset!</b>.\n",
    "- Your solution might be considered as the \"copy\" of others solutions, in that specific case the resulting score for all involved students will be 0/8.\n",
    "- Then read carefully all the part of the jupyter notebook and fill all fields.\n",
    "- <b>solutions (and correspondig points) are evaluated mainly on your thoughts/comments/opinions</b>.  \n",
    "- If you have questions <b>Don't write \"personal\" emails</b> to Stefano Faralli, instead <b>use our google group</b>.\n",
    "- A solution having a summary discussion with less than 500 words is evaluated with 0 points.\n",
    "- Comments summary etc.. must be in <b>English</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "### Dataset and task Description:\n",
    "<img width='400' src='news-online.jpeg'/>\n",
    "\n",
    "- The challenge is about online news popularity;\n",
    "- The provided dataset consists of one single csv file (\"OnlineNewsPopularity.csv\");\n",
    "- The provided dataset is a modified <b>noisy</b> version of the original dataset described in [1];\n",
    "\n",
    "[1] K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision\n",
    "    Support System for Predicting the Popularity of Online News. Proceedings\n",
    "    of the 17th EPIA 2015 - Portuguese Conference on Artificial Intelligence,\n",
    "    September, Coimbra, Portugal\n",
    "\n",
    "\n",
    "This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal of the task is to predict the number of shares in social networks (popularity).\n",
    "\n",
    "Number of Instances: <b>39,797</b> \n",
    "\n",
    "Number of Attributes: <b>61</b>\n",
    "\n",
    "Target: <b>shares</b>\n",
    "\n",
    "### Attribute Information:\n",
    "\n",
    "<table>\n",
    " <tr><th> index </th><th>name</th><th>description</th></tr>\n",
    " <tr><td>0</td><td>url</td><td>URL of the article</td></tr>\n",
    " <tr><td>1</td><td>timedelta</td><td>Days between the article publication and the dataset acquisition</td></tr>\n",
    " <tr><td>2</td><td>n_tokens_title</td><td>Number of words in the title</td></tr>\n",
    " <tr><td>3</td><td>n_tokens_content</td><td>Number of words in the content</td></tr>\n",
    " <tr><td>4</td><td>n_unique_tokens</td><td>Rate of unique words in the content</td></tr>\n",
    " <tr><td>5</td><td>n_non_stop_words</td><td>Rate of non-stop words in the content</td></tr>\n",
    " <tr><td>6</td><td>n_non_stop_unique_tokens</td><td>Rate of unique non-stop words in content</td></tr>\n",
    " <tr><td>7</td><td>num_hrefs</td><td>Number of links</td></tr>\n",
    " <tr><td>8</td><td>num_self_hrefs</td><td>Number of links to other articles published by Mashable</td></tr>\n",
    " <tr><td>9</td><td>num_imgs</td><td>Number of images</td></tr>\n",
    " <tr><td>10</td><td>num_videos</td><td>Number of videos</td></tr>\n",
    " <tr><td>11</td><td>average_token_length</td><td>Average length of the words in the content</td></tr>\n",
    " <tr><td>12</td><td>num_keywords</td><td>Number of keywords in the metadata</td></tr>\n",
    " <tr><td>13</td><td>data_channel_is_lifestyle</td><td>Is data channel 'Lifestyle'?</td></tr>\n",
    " <tr><td>14</td><td>data_channel_is_entertainment</td><td>Is data channel 'Entertainment'?</td></tr>\n",
    " <tr><td>15</td><td>data_channel_is_bus</td><td>Is data channel 'Business'?</td></tr>\n",
    " <tr><td>16</td><td>data_channel_is_socmed</td><td>Is data channel 'Social Media'?</td></tr>\n",
    " <tr><td>17</td><td>data_channel_is_tech</td><td>Is data channel 'Tech'?</td></tr>\n",
    " <tr><td>18</td><td>data_channel_is_world</td><td>Is data channel 'World'?</td></tr>\n",
    " <tr><td>19</td><td>kw_min_min</td><td>Worst keyword (min. shares)</td></tr>\n",
    " <tr><td>20</td><td>kw_max_min</td><td>Worst keyword (max. shares)</td></tr>\n",
    " <tr><td>21</td><td>kw_avg_min</td><td>Worst keyword (avg. shares)</td></tr>\n",
    " <tr><td>22</td><td>kw_min_max</td><td>Best keyword (min. shares)</td></tr>\n",
    " <tr><td>23</td><td>kw_max_max</td><td>Best keyword (max. shares)</td></tr>\n",
    " <tr><td>24</td><td>kw_avg_max</td><td>Best keyword (avg. shares)</td></tr>\n",
    " <tr><td>25</td><td>kw_min_avg</td><td>Avg. keyword (min. shares)</td></tr>\n",
    " <tr><td>26</td><td>kw_max_avg</td><td>Avg. keyword (max. shares)</td></tr>\n",
    " <tr><td>27</td><td>kw_avg_avg</td><td>Avg. keyword (avg. shares)</td></tr>\n",
    " <tr><td>28</td><td>self_reference_min_shares</td><td>Min. shares of referenced articles in Mashable</td></tr>\n",
    " <tr><td>29</td><td>self_reference_max_shares</td><td>Max. shares of referenced articles in Mashable</td></tr>\n",
    " <tr><td>30</td><td>self_reference_avg_sharess</td><td>Avg. shares of referenced articles in Mashable</td></tr>\n",
    " <tr><td>31</td><td>weekday_is_monday</td><td>Was the article published on a Monday?</td></tr>\n",
    " <tr><td>32</td><td>weekday_is_tuesday</td><td>Was the article published on a Tuesday?</td></tr>\n",
    " <tr><td>33</td><td>weekday_is_wednesday</td><td>Was the article published on a Wednesday?</td></tr>\n",
    " <tr><td>34</td><td>weekday_is_thursday</td><td>Was the article published on a Thursday?</td></tr>\n",
    " <tr><td>35</td><td>weekday_is_friday</td><td>Was the article published on a Friday?</td></tr>\n",
    " <tr><td>36</td><td>weekday_is_saturday</td><td>Was the article published on a Saturday?</td></tr>\n",
    " <tr><td>37</td><td>weekday_is_sunday</td><td> Was the article published on a Sunday?</td></tr>\n",
    " <tr><td>38</td><td>is_weekend</td><td>Was the article published on the weekend?</td></tr>\n",
    " <tr><td>39</td><td>LDA_00</td><td>Closeness to LDA topic 0</td></tr>\n",
    " <tr><td>40</td><td>LDA_01</td><td>Closeness to LDA topic 1</td></tr>\n",
    " <tr><td>41</td><td>LDA_02</td><td>Closeness to LDA topic 2</td></tr>\n",
    " <tr><td>42</td><td>LDA_03</td><td>Closeness to LDA topic 3</td></tr>\n",
    " <tr><td>43</td><td>LDA_04</td><td>Closeness to LDA topic 4</td></tr>\n",
    " <tr><td>44</td><td>global_subjectivity</td><td>Text subjectivity</td></tr>\n",
    " <tr><td>45</td><td>global_sentiment_polarity</td><td>Text sentiment polarity</td></tr>\n",
    " <tr><td>46</td><td>global_rate_positive_words</td><td>Rate of positive words in the content</td></tr>\n",
    " <tr><td>47</td><td>global_rate_negative_words</td><td> Rate of negative words in the content</td></tr>\n",
    " <tr><td>48</td><td>rate_positive_words</td><td>Rate of positive words among non-neutral tokens</td></tr>\n",
    " <tr><td>49</td><td>rate_negative_words</td><td>Rate of negative words among non-neutral tokens</td></tr>\n",
    " <tr><td>50</td><td>avg_positive_polarity</td><td>Avg. polarity of positive words</td></tr>\n",
    " <tr><td>51</td><td>min_positive_polarity</td><td>Min. polarity of positive words</td></tr>\n",
    " <tr><td>52</td><td>max_positive_polarity</td><td>Max. polarity of positive words</td></tr>\n",
    " <tr><td>53</td><td>avg_negative_polarity</td><td>Avg. polarity of negative words</td></tr>\n",
    " <tr><td>54</td><td>min_negative_polarity</td><td>Min. polarity of negative words</td></tr>\n",
    " <tr><td>55</td><td>max_negative_polarity</td><td>Max. polarity of negative words</td></tr>\n",
    " <tr><td>56</td><td>title_subjectivity</td><td>Title subjectivity</td></tr>\n",
    " <tr><td>57</td><td>title_sentiment_polarity</td><td>Title polarity</td></tr>\n",
    " <tr><td>58</td><td>abs_title_subjectivity</td><td>Absolute subjectivity level</td></tr>\n",
    " <tr><td>59</td><td>abs_title_sentiment_polarity</td><td>Absolute polarity level</td></tr>\n",
    "     <tr><td>60</td><td>shares</td><td>Number of shares (target)</td></tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# 2. Pre-processing (up to 3 of 11.2 points)     \n",
    "     \n",
    "     \n",
    "## 2.1 Clean and Load the Dataset (up to 1 of 11.2 points)\n",
    "Use the following two cells (a code cell and, a markdown cell) to: \n",
    "- create a pandas DataFrame by loading a cleaned version of the \"OnlineNewsPopularity.cvs\" file.  \n",
    "- describe the identified noise and the methodology used to fix the problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of rows after skip row number 39537: 39647\n",
      "The number of rows after remove rows with NaN value: 39639\n",
      "9\n",
      "10\n",
      "37\n",
      "________________________\n",
      "columns type: \n",
      "url                              object\n",
      "timedelta                       float64\n",
      "n_tokens_title                  float64\n",
      "n_tokens_content                float64\n",
      "n_unique_tokens                 float64\n",
      "                                 ...   \n",
      "title_subjectivity              float64\n",
      "title_sentiment_polarity        float64\n",
      "abs_title_subjectivity          float64\n",
      "abs_title_sentiment_polarity    float64\n",
      "shares                            int64\n",
      "Length: 61, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39629</th>\n",
       "      <td>http://mashable.com/2014/12/27/nypd-rafael-ram...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1629.0</td>\n",
       "      <td>0.425711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.606092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.429534</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39630</th>\n",
       "      <td>http://mashable.com/2014/12/27/protests-contin...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>0.653153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.825758</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39631</th>\n",
       "      <td>http://mashable.com/2014/12/27/samsung-app-aut...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>0.529052</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.684783</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.260000</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39632</th>\n",
       "      <td>http://mashable.com/2014/12/27/seth-rogen-jame...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>0.696296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.885057</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.211111</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39633</th>\n",
       "      <td>http://mashable.com/2014/12/27/ukraine-blasts/</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>682.0</td>\n",
       "      <td>0.539493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.692661</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.205246</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39634 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url  timedelta  \\\n",
       "0      http://mashable.com/2013/01/07/amazon-instant-...      731.0   \n",
       "1      http://mashable.com/2013/01/07/ap-samsung-spon...      731.0   \n",
       "2      http://mashable.com/2013/01/07/apple-40-billio...      731.0   \n",
       "3      http://mashable.com/2013/01/07/astronaut-notre...      731.0   \n",
       "4       http://mashable.com/2013/01/07/att-u-verse-apps/      731.0   \n",
       "...                                                  ...        ...   \n",
       "39629  http://mashable.com/2014/12/27/nypd-rafael-ram...        8.0   \n",
       "39630  http://mashable.com/2014/12/27/protests-contin...        8.0   \n",
       "39631  http://mashable.com/2014/12/27/samsung-app-aut...        8.0   \n",
       "39632  http://mashable.com/2014/12/27/seth-rogen-jame...        8.0   \n",
       "39633     http://mashable.com/2014/12/27/ukraine-blasts/        8.0   \n",
       "\n",
       "       n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0                12.0             219.0         0.663594               1.0   \n",
       "1                 9.0             255.0         0.604743               1.0   \n",
       "2                 9.0             211.0         0.575130               1.0   \n",
       "3                 9.0             531.0         0.503788               1.0   \n",
       "4                13.0            1072.0         0.415646               1.0   \n",
       "...               ...               ...              ...               ...   \n",
       "39629            13.0            1629.0         0.425711               1.0   \n",
       "39630            11.0             223.0         0.653153               1.0   \n",
       "39631            11.0             346.0         0.529052               1.0   \n",
       "39632            12.0             328.0         0.696296               1.0   \n",
       "39633             6.0             682.0         0.539493               1.0   \n",
       "\n",
       "       n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  ...  \\\n",
       "0                      0.815385        4.0             2.0       1.0  ...   \n",
       "1                      0.791946        3.0             1.0       1.0  ...   \n",
       "2                      0.663866        3.0             1.0       1.0  ...   \n",
       "3                      0.665635        9.0             0.0       1.0  ...   \n",
       "4                      0.540890       19.0            19.0      20.0  ...   \n",
       "...                         ...        ...             ...       ...  ...   \n",
       "39629                  0.606092       15.0            12.0       6.0  ...   \n",
       "39630                  0.825758        5.0             3.0       1.0  ...   \n",
       "39631                  0.684783        9.0             7.0       1.0  ...   \n",
       "39632                  0.885057        9.0             7.0       3.0  ...   \n",
       "39633                  0.692661       10.0             1.0       1.0  ...   \n",
       "\n",
       "       min_positive_polarity  max_positive_polarity  avg_negative_polarity  \\\n",
       "0                   0.100000                   0.70              -0.350000   \n",
       "1                   0.033333                   0.70              -0.118750   \n",
       "2                   0.100000                   1.00              -0.466667   \n",
       "3                   0.136364                   0.80              -0.369697   \n",
       "4                   0.033333                   1.00              -0.220192   \n",
       "...                      ...                    ...                    ...   \n",
       "39629               0.033333                   1.00              -0.429534   \n",
       "39630               0.214286                   0.80              -0.250000   \n",
       "39631               0.100000                   0.75              -0.260000   \n",
       "39632               0.136364                   0.70              -0.211111   \n",
       "39633               0.062500                   0.50              -0.205246   \n",
       "\n",
       "       min_negative_polarity  max_negative_polarity  title_subjectivity  \\\n",
       "0                     -0.600              -0.200000            0.500000   \n",
       "1                     -0.125              -0.100000            0.000000   \n",
       "2                     -0.800              -0.133333            0.000000   \n",
       "3                     -0.600              -0.166667            0.000000   \n",
       "4                     -0.500              -0.050000            0.454545   \n",
       "...                      ...                    ...                 ...   \n",
       "39629                 -1.000              -0.050000            0.783333   \n",
       "39630                 -0.250              -0.250000            0.000000   \n",
       "39631                 -0.500              -0.125000            0.100000   \n",
       "39632                 -0.400              -0.100000            0.300000   \n",
       "39633                 -0.500              -0.012500            0.000000   \n",
       "\n",
       "       title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                     -0.187500                0.000000   \n",
       "1                      0.000000                0.500000   \n",
       "2                      0.000000                0.500000   \n",
       "3                      0.000000                0.500000   \n",
       "4                      0.136364                0.045455   \n",
       "...                         ...                     ...   \n",
       "39629                 -0.600000                0.283333   \n",
       "39630                  0.000000                0.500000   \n",
       "39631                  0.000000                0.400000   \n",
       "39632                  1.000000                0.200000   \n",
       "39633                  0.000000                0.500000   \n",
       "\n",
       "       abs_title_sentiment_polarity  shares  \n",
       "0                          0.187500     593  \n",
       "1                          0.000000     711  \n",
       "2                          0.000000    1500  \n",
       "3                          0.000000    1200  \n",
       "4                          0.136364     505  \n",
       "...                             ...     ...  \n",
       "39629                      0.600000    1400  \n",
       "39630                      0.000000    1200  \n",
       "39631                      0.000000    1800  \n",
       "39632                      1.000000    1900  \n",
       "39633                      0.000000    1100  \n",
       "\n",
       "[39634 rows x 61 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# write here the code for 2.1 Load the Dataset\n",
    "# Import libraries for load and clean data\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "\n",
    "#To avoid warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Dataset filename for load\n",
    "file_name = \"OnlineNewsPopularity.csv\"\n",
    "\n",
    "# 1 line is corrupted then used skip\n",
    "# ParserError: Error tokenizing data. C error: Expected 61 fields in line 39537, saw 62\n",
    "# row number 39537 has 62 columns then we ignore it.\n",
    "# ref: https://stackoverflow.com/questions/18039057/python-pandas-error-tokenizing-data\n",
    "df = pd.read_csv(file_name, on_bad_lines='skip')\n",
    "print(\"The total number of rows after skip row number 39537:\", df.shape[0])\n",
    "\n",
    "# ref: https://www.codegrepper.com/code-examples/python/pandas+remove+spaces+in+the+column+names\n",
    "# remove spaces in columns name\n",
    "df.columns = df.columns.str.replace(' ','')\n",
    "\n",
    "# ref: https://sparkbyexamples.com/pandas/pandas-drop-rows-with-nan-values-in-dataframe/\n",
    "# clean dataset: remove rows with NaN value, remove rows with wrong url format.\n",
    "# drop rows with NaN values in DataFrame (missing values)\n",
    "df.dropna(inplace=True)\n",
    "print(\"The number of rows after remove rows with NaN value:\", df.shape[0])\n",
    "\n",
    "# Reset index after drop\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# ref: https://sparkbyexamples.com/pandas/pandas-get-first-row-value-of-a-given-column/\n",
    "# get ith value in the 'title' column: df['title'].iloc[i]\n",
    "# Using column Index \n",
    "\n",
    "# column index value of \"Name\" column is 0\n",
    "# We have set takeable = True\n",
    "# to interpret the index / col as indexer\n",
    "# df.get_value(4, 0, takeable = True)\n",
    "\n",
    "# print('columns type: ')\n",
    "# print(df.dtypes)\n",
    "\n",
    "# Check rows has valid url?\n",
    "for i in df['url']:\n",
    "    if i[:7] != 'http://':\n",
    "        print(i)\n",
    "# all rows has a valid url.\n",
    "\n",
    "# Check specific columns to have positive value:\n",
    "# for i in range(1, 52):\n",
    "#     for j in df[df.columns[i]]:\n",
    "#         if j<0:\n",
    "#             print(j)\n",
    "# for i in df['abs_title_subjectivity']:\n",
    "#     if i<0:\n",
    "#         print(i)\n",
    "# for i in df['abs_title_sentiment_polarity']:\n",
    "#     if i<0:\n",
    "#         print(i)\n",
    "\n",
    "\n",
    "# Unable to parse string \" n.a.\" at position 37907\n",
    "# Then remove this row from dataframe\n",
    "df.drop(37907, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# row nmuber 32768 in shares column is str, then convert to int\n",
    "# Convert 'shares' column to numeric\n",
    "df['shares'] = pd.to_numeric(df['shares'])\n",
    "\n",
    "# Remove row has 'n.a.' value in column number 9. (38059)\n",
    "df.drop(38059, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# Remove row has 'n.a.' value in column number 10. (36554, 38173)\n",
    "df.drop(36554, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.drop(38173, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# Remove row has 'n.a.' value in column number 37. (35154)\n",
    "df.drop(35154, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Convert columns with object data type to numeric\n",
    "for i in range(1, 61):\n",
    "    if df.dtypes[i] == 'object' or df.dtypes[i] == 'O':\n",
    "        print(i)\n",
    "        df[df.columns[i]] = pd.to_numeric(df[df.columns[i]])\n",
    "\n",
    "# Check shares column values has positive value.\n",
    "for i in df['shares']:\n",
    "    if i<0:\n",
    "        print(i)\n",
    "\n",
    "print('________________________')\n",
    "\n",
    "# Check every column in right format\n",
    "# Rate should have a value between 0 to 1.\n",
    "for i in df['rate_negative_words']:\n",
    "    if not 0<=i<=1:\n",
    "        print(i)\n",
    "for i in df['rate_negative_words']:\n",
    "    if not 0<=i<=1:\n",
    "        print(i)\n",
    "for i in df['global_rate_negative_words']:\n",
    "    if not 0<=i<=1:\n",
    "        print(i)\n",
    "for i in df['global_rate_positive_words']:\n",
    "    if not 0<=i<=1:\n",
    "        print(i)\n",
    "\n",
    "\n",
    "# Check bool atts:\n",
    "temp_start_index = list(df.columns).index('data_channel_is_lifestyle')\n",
    "temp_end_index = list(df.columns).index('data_channel_is_world')\n",
    "for j in range(temp_start_index, temp_end_index + 1):\n",
    "    for i in df[df.columns[j]]:\n",
    "        if not (int(i)==1 or int(i)==0):\n",
    "            print(i)\n",
    "\n",
    "temp_start_index = list(df.columns).index('weekday_is_monday')\n",
    "temp_end_index = list(df.columns).index('is_weekend')\n",
    "for j in range(temp_start_index, temp_end_index + 1):\n",
    "    for i in df[df.columns[j]]:\n",
    "        if not (int(i)==1 or int(i)==0):\n",
    "            print(i)\n",
    "\n",
    "# Display dataset in a dataframe and data tpye of columns\n",
    "print('columns type: ')\n",
    "# print(list(df.dtypes))\n",
    "print(df.dtypes)\n",
    "display(df)\n",
    "\n",
    "# Save Cleaned dataset to file\n",
    "df.to_csv('OnlineNewsPopularity Clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Clean & Load Dataset</h1>\n",
    "<h3>After clean dataset we saved dataset in csv file with name \"OnlineNewsPopularity Clean.csv\"</h3>\n",
    "<h2>Fix headers</h2>\n",
    "remove spaces in columns header, for example convert ' heder' to 'header'.\n",
    "<h2>Remove corrupted rows</h2>\n",
    "<h3>row with wrong shape</h3>\n",
    "I use 'pd.read_csv' for read data from csv file, and one line is corrupted, then i used skip (on_bad_lines='skip')\n",
    "ParserError: Error tokenizing data. C error: Expected 61 fields in line 39537, saw 62\n",
    "error said row number 39537 has 62 columns (but dataset has 61 columns) then we ignore it.\n",
    "<h3>rows with NaN values</h3>\n",
    "remove rows with NaN value, remove rows with wrong url format. with df.dropna(inplace=True)\n",
    "inplace=True: replace dataframe after do the function.\n",
    "\n",
    "After every step remove rows we should update row index with this code: df.reset_index(drop=True, inplace=True)\n",
    "To get ith value in the 'title' column: df['title'].iloc[i]\n",
    "\n",
    "<h3>Check url form</h3>\n",
    "all rows has a valid url.\n",
    "\n",
    "<h3>Columns datatype</h3>\n",
    "to print columns datatypes: print(df.dtypes)\n",
    "face with errors of convert columns with object data type to numeric exept first(url) column\n",
    "when error occured we fix the problem and remove that row.\n",
    "in this dataset we have ' n.a.' values, then we remove rows with this values.\n",
    "in columns: 9, 10, 37\n",
    "then convert columns with object data type to numeric.\n",
    "\n",
    "<h3>check columns value with metadata</h3>\n",
    "number of shares should have positive values, then we remove rows with shares negative value.\n",
    "values in columns that about \"rate\" should have a value between 0 and 1.\n",
    "Check bool atts: boolean columns (columns thats has \"is\" in header) should have 0 or 1 value. like \"data_channel_is_lifestyle\" and weekday_is_monday ...\n",
    "\n",
    "<h2>Visualize data and datatypes</h2>\n",
    "then visualize dataframe and dataframe columns data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean dataset\n",
    "df = pd.read_csv('OnlineNewsPopularity Clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Dataset Analysis (up to 1 of 11.2 points)\n",
    "In the following code cell (feel free to create new cells), remember to comment your code snippets:\n",
    "\n",
    "1) Print the total number of samples;\n",
    "\n",
    "2) Print a table with the first 15 samples;\n",
    "\n",
    "3) Plot the histogram distribution of \"shares\";\n",
    "\n",
    "4) A bar chart counting the attributes:  data_channel_is_lifestyle, data_channel_is_entertainment, data_channel_is_bus, data_channel_is_socmed, data_channel_is_tech, data_channel_is_world;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of samples= 39634\n"
     ]
    }
   ],
   "source": [
    "# write here the code for 2.2 Dataset Analysis\n",
    "\n",
    "# ref: https://stackoverflow.com/questions/15943769/how-do-i-get-the-row-count-of-a-pandas-dataframe\n",
    "# 1\n",
    "print('The total number of samples=', df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.6000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.1250</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.8000</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.6000</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://mashable.com/2013/01/07/beewi-smart-toys/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>0.559889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.698198</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.195000</td>\n",
       "      <td>-0.4000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://mashable.com/2013/01/07/bodymedia-armba...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>960.0</td>\n",
       "      <td>0.418163</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.549834</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.224479</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://mashable.com/2013/01/07/canon-poweshot-n/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>989.0</td>\n",
       "      <td>0.433574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.572108</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.242778</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://mashable.com/2013/01/07/car-of-the-futu...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.670103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>-0.1250</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://mashable.com/2013/01/07/chuck-hagel-web...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.238095</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>http://mashable.com/2013/01/07/cosmic-events-d...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1248.0</td>\n",
       "      <td>0.490050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.731638</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.415064</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>http://mashable.com/2013/01/07/crayon-creatures/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.262500</td>\n",
       "      <td>-0.4000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>http://mashable.com/2013/01/07/creature-cups/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>0.609195</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.707602</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.310417</td>\n",
       "      <td>-0.6000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>http://mashable.com/2013/01/07/dad-jokes/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.841530</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.337889</td>\n",
       "      <td>-0.7000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>http://mashable.com/2013/01/07/downton-abbey-t...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>0.562753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.138690</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  url  timedelta  \\\n",
       "0   http://mashable.com/2013/01/07/amazon-instant-...      731.0   \n",
       "1   http://mashable.com/2013/01/07/ap-samsung-spon...      731.0   \n",
       "2   http://mashable.com/2013/01/07/apple-40-billio...      731.0   \n",
       "3   http://mashable.com/2013/01/07/astronaut-notre...      731.0   \n",
       "4    http://mashable.com/2013/01/07/att-u-verse-apps/      731.0   \n",
       "5    http://mashable.com/2013/01/07/beewi-smart-toys/      731.0   \n",
       "6   http://mashable.com/2013/01/07/bodymedia-armba...      731.0   \n",
       "7    http://mashable.com/2013/01/07/canon-poweshot-n/      731.0   \n",
       "8   http://mashable.com/2013/01/07/car-of-the-futu...      731.0   \n",
       "9   http://mashable.com/2013/01/07/chuck-hagel-web...      731.0   \n",
       "10  http://mashable.com/2013/01/07/cosmic-events-d...      731.0   \n",
       "11   http://mashable.com/2013/01/07/crayon-creatures/      731.0   \n",
       "12      http://mashable.com/2013/01/07/creature-cups/      731.0   \n",
       "13          http://mashable.com/2013/01/07/dad-jokes/      731.0   \n",
       "14  http://mashable.com/2013/01/07/downton-abbey-t...      731.0   \n",
       "\n",
       "    n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0             12.0             219.0         0.663594               1.0   \n",
       "1              9.0             255.0         0.604743               1.0   \n",
       "2              9.0             211.0         0.575130               1.0   \n",
       "3              9.0             531.0         0.503788               1.0   \n",
       "4             13.0            1072.0         0.415646               1.0   \n",
       "5             10.0             370.0         0.559889               1.0   \n",
       "6              8.0             960.0         0.418163               1.0   \n",
       "7             12.0             989.0         0.433574               1.0   \n",
       "8             11.0              97.0         0.670103               1.0   \n",
       "9             10.0             231.0         0.636364               1.0   \n",
       "10             9.0            1248.0         0.490050               1.0   \n",
       "11            10.0             187.0         0.666667               1.0   \n",
       "12             9.0             274.0         0.609195               1.0   \n",
       "13             9.0             285.0         0.744186               1.0   \n",
       "14             8.0             259.0         0.562753               1.0   \n",
       "\n",
       "    n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  ...  \\\n",
       "0                   0.815385        4.0             2.0       1.0  ...   \n",
       "1                   0.791946        3.0             1.0       1.0  ...   \n",
       "2                   0.663866        3.0             1.0       1.0  ...   \n",
       "3                   0.665635        9.0             0.0       1.0  ...   \n",
       "4                   0.540890       19.0            19.0      20.0  ...   \n",
       "5                   0.698198        2.0             2.0       0.0  ...   \n",
       "6                   0.549834       21.0            20.0      20.0  ...   \n",
       "7                   0.572108       20.0            20.0      20.0  ...   \n",
       "8                   0.836735        2.0             0.0       0.0  ...   \n",
       "9                   0.797101        4.0             1.0       1.0  ...   \n",
       "10                  0.731638       11.0             0.0       1.0  ...   \n",
       "11                  0.800000        7.0             0.0       1.0  ...   \n",
       "12                  0.707602       18.0             2.0      11.0  ...   \n",
       "13                  0.841530        4.0             2.0       0.0  ...   \n",
       "14                  0.644444       19.0             3.0       9.0  ...   \n",
       "\n",
       "    min_positive_polarity  max_positive_polarity  avg_negative_polarity  \\\n",
       "0                0.100000                    0.7              -0.350000   \n",
       "1                0.033333                    0.7              -0.118750   \n",
       "2                0.100000                    1.0              -0.466667   \n",
       "3                0.136364                    0.8              -0.369697   \n",
       "4                0.033333                    1.0              -0.220192   \n",
       "5                0.136364                    0.6              -0.195000   \n",
       "6                0.100000                    1.0              -0.224479   \n",
       "7                0.100000                    1.0              -0.242778   \n",
       "8                0.400000                    0.8              -0.125000   \n",
       "9                0.100000                    0.5              -0.238095   \n",
       "10               0.100000                    1.0              -0.415064   \n",
       "11               0.200000                    0.7              -0.262500   \n",
       "12               0.200000                    0.7              -0.310417   \n",
       "13               0.160000                    1.0              -0.337889   \n",
       "14               0.136364                    0.5              -0.138690   \n",
       "\n",
       "    min_negative_polarity  max_negative_polarity  title_subjectivity  \\\n",
       "0                 -0.6000              -0.200000            0.500000   \n",
       "1                 -0.1250              -0.100000            0.000000   \n",
       "2                 -0.8000              -0.133333            0.000000   \n",
       "3                 -0.6000              -0.166667            0.000000   \n",
       "4                 -0.5000              -0.050000            0.454545   \n",
       "5                 -0.4000              -0.100000            0.642857   \n",
       "6                 -0.5000              -0.050000            0.000000   \n",
       "7                 -0.5000              -0.050000            1.000000   \n",
       "8                 -0.1250              -0.125000            0.125000   \n",
       "9                 -0.5000              -0.100000            0.000000   \n",
       "10                -1.0000              -0.100000            0.000000   \n",
       "11                -0.4000              -0.125000            0.000000   \n",
       "12                -0.6000              -0.050000            1.000000   \n",
       "13                -0.7000              -0.100000            1.000000   \n",
       "14                -0.1875              -0.050000            0.750000   \n",
       "\n",
       "    title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                  -0.187500                0.000000   \n",
       "1                   0.000000                0.500000   \n",
       "2                   0.000000                0.500000   \n",
       "3                   0.000000                0.500000   \n",
       "4                   0.136364                0.045455   \n",
       "5                   0.214286                0.142857   \n",
       "6                   0.000000                0.500000   \n",
       "7                   0.500000                0.500000   \n",
       "8                   0.000000                0.375000   \n",
       "9                   0.000000                0.500000   \n",
       "10                  0.000000                0.500000   \n",
       "11                  0.000000                0.500000   \n",
       "12                 -1.000000                0.500000   \n",
       "13                 -1.000000                0.500000   \n",
       "14                  0.550000                0.250000   \n",
       "\n",
       "    abs_title_sentiment_polarity  shares  \n",
       "0                       0.187500     593  \n",
       "1                       0.000000     711  \n",
       "2                       0.000000    1500  \n",
       "3                       0.000000    1200  \n",
       "4                       0.136364     505  \n",
       "5                       0.214286     855  \n",
       "6                       0.000000     556  \n",
       "7                       0.500000     891  \n",
       "8                       0.000000    3600  \n",
       "9                       0.000000     710  \n",
       "10                      0.000000    2200  \n",
       "11                      0.000000    1900  \n",
       "12                      1.000000     823  \n",
       "13                      1.000000   10000  \n",
       "14                      0.550000     761  \n",
       "\n",
       "[15 rows x 61 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2\n",
    "display(df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Distribution of shares histogram')\n",
    "plt.hist(df['shares'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2099.0, 7054.0, 6258.0, 2323.0, 7345.0, 8424.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAE4CAYAAAC37COOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAp1klEQVR4nO3de5xdVX3+8c9juIVL5JYgJoGgRixQRYkB7xdQQNAgNRoUExRNQVBRrATtr2LbVGqtVlrBUrWEqmBqtUQUNabSForggEgERKJcEoJkBJEoSrk8vz/WGjydzGTOwMw5M7Of9+s1r7Nn7b3PWXvmnO9eZ11lm4iIaIYndDsDERHROQn6ERENkqAfEdEgCfoREQ2SoB8R0SAJ+hERDZKgHx0j6QxJn+92PvqTdKmkt3U7HyOhm39jSZb0tG68drQvQX+CkXSrpN9K2ijpXkn/I+kESW39ryXNqh/eLUY7rzE8Tfvf1PfyId3Ox0SToD8xvdr2DsCewJnAacBnu5ulGG+6dXNpyk2tWxL0JzDbv7K9AngDsEjSfgCSjpD0A0n3SVor6YyW0/6rPt4r6deSnifpqZL+Q9Ldkn4h6QuSdhzsdSXtK2mlpHsk3SXpAy27t5J0fv0mcr2kOS3nLZH007rvBkmvbdl3nKTLJH1M0i8l3SLp8Jb9l0r6C0mX1/O/LWnXlv0H1W8990r6oaSXDpL3p0n6T0m/qtf6pc1c5wtbnnOtpONq+hPrNfZKuk3Sn/Z90+pf/dK/9D7EdWzyvxkka9tI+lI9/xpJzxrG3/hySZ+QdA9wxgDXPEnSB1qe42pJM1sOOUTSzfV/9ClJqudt9j1US/WnSboO+I2kC4A9gK/Va33/YP+HGCbb+ZlAP8CtwCEDpN8OnFi3Xwr8IeWm/0zgLuCoum8WYGCLlnOfBrwC2BqYSgk+fzfI6+8A3AmcCmxTfz+w7jsD+B3wKmAS8BHgey3nzgeeXPP1BuA3wO5133HAg8Db67knAusB1f2XAj8Fng5Mrr+fWfdNB+6ur/uEei13A1Nbzn1b3b4A+GA9bhvghYNc5x7ARuAYYEtgF2D/uu984KJ67bOAnwDHt/wNPt/yPP/n7z3EdWzyvxkgX2fUv9Prar7eB9wCbNnm3/gh4J3AFsDkAZ7/T4DVwN6AgGcBu9R9Bi4Gdqx/n17gsHbeQ5T37bXAzL7XZZD3cn4e309K+s2xHtgZwPaltlfbfsT2dZRA95LBTrS9xvZK2w/Y7gU+vpnjjwR+bvtvbf/O9kbbV7bsv8z2N2w/DPwLJWj0vc6/2l5f8/Ul4GZgbsu5t9n+p3ruMmB3YLeW/f9s+ye2fwssB/av6ccC36iv+4jtlUAP5SbQ34OUarEn1/xfNsh1vgn4ju0LbD9o+27b10qaRAmmp9drvxX4W+DNgzzPQAa7jnZdbfvLth+k/K+2AQ6Ctv7G623/ve2H6uv39zbgT23f5OKHtu9u2X+m7Xtt3w58ty/vbb6HzrK9dpDXjRGSoN8c04F7ACQdKOm7tfrhV8AJwK6DnShpmqQLJd0h6T7g85s5fialpDqYn7ds30+piuir2lgo6dpaXXIvsF+/13n0XNv3183tN/Pcffv2BOb3PW997hdSbhr9vZ9Sgr2qVj+9dZDrGOw6dwW2Am5rSbuN8vdv12DX0a61fRu2HwHWUUr37fyN17J5w/3/bl9ft5330FCvHSMgQb8BJD2XEnT6Sq1fBFYAM20/Efg0JdBB+Yre30dq+jNtT6GUnDXAcVA+uE99DHncE/gn4GRKdcGOwI828zrDsRb4F9s7tvxsZ/vM/gfa/rntt9t+MvDHwNkauBviYNf5C37/baHPHsAddfs3wLYt+540jOtod0rcR+vYa1vCDGB9m3/joV7jMf1/ae891P+1MwXwKEjQn8AkTZF0JHAhpR55dd21A3CP7d9Jmgu8seW0XuAR4CktaTsAv6Y0IE6n1OsO5mLgSZJOkbS1pB0kHdhGdrejfMh7a97fQimFjoTPA6+WdGhtiNxG0kslzeh/oKT5Lem/rHl6eIDn/AKl0fL1kraQtIuk/WvV03Jgab32PYH31jxAqbd+saQ9JD0ROH0Y1zHQ/2YgB0g6un6DOgV4APgeI/M3/gzwF5Jmq3impF3aOG8476E+dzH0tcYwJehPTF+TtJFSKvsgpf70LS373wH8eT3mzyhBCni02mQpcHmtAjgI+DDwHOBXwNeBrwz2wrY3UhrsXk35qn8z8LKhMmz7Bkrd9xWUD/sfApe3eb1DPfdaYB7wAUrAW0sJOgO9/58LXCnp15RvQ++2fcsAz3k7pU3gVEq12bX8vn3inZQS/c8o366+CHyunrcS+BJwHXA15SbZ7nUM9L8ZyEWUdoVfUtoSjq7tDiPxN/445f3ybeA+SlfgyW2c1/Z7qMVHgD+t1/q+YeYzBtHX8yEiIhogJf2IiAZJ0I+IaJAE/YiIBknQj4hokDE/sdGuu+7qWbNmdTsbERHjytVXX/0L21P7p4/5oD9r1ix6enq6nY2IiHFF0m0Dpad6JyKiQRL0IyIaJEE/IqJBEvQjIhokQT8iokES9CMiGiRBPyKiQRL0IyIaJEE/IqJBxvyI3IiIkTRryde7nYW23HrmEaPyvCnpR0Q0SIJ+RESDJOhHRDRIgn5ERIMk6EdENEiCfkREg7QV9CW9R9L1kn4k6QJJ20jaWdJKSTfXx51ajj9d0hpJN0k6tCX9AEmr676zJGk0LioiIgY2ZNCXNB14FzDH9n7AJGABsARYZXs2sKr+jqR96v59gcOAsyVNqk93DrAYmF1/DhvRq4mIiM1qt3pnC2CypC2AbYH1wDxgWd2/DDiqbs8DLrT9gO1bgDXAXEm7A1NsX2HbwPkt50RERAcMGfRt3wF8DLgduBP4le1vA7vZvrMecycwrZ4yHVjb8hTratr0ut0/fROSFkvqkdTT29s7vCuKiIhBtVO9sxOl9L4X8GRgO0nHbu6UAdK8mfRNE+1zbc+xPWfq1E0Wc4+IiMeoneqdQ4BbbPfafhD4CvB84K5aZUN93FCPXwfMbDl/BqU6aF3d7p8eEREd0k7Qvx04SNK2tbfNwcCNwApgUT1mEXBR3V4BLJC0taS9KA22V9UqoI2SDqrPs7DlnIiI6IAhZ9m0faWkLwPXAA8BPwDOBbYHlks6nnJjmF+Pv17ScuCGevxJth+uT3cicB4wGbik/kRERIe0NbWy7Q8BH+qX/ACl1D/Q8UuBpQOk9wD7DTOPERExQjIiNyKiQRL0IyIaJEE/IqJBEvQjIhokQT8iokES9CMiGiRBPyKiQRL0IyIaJEE/IqJBEvQjIhqkrWkYIqK5Zi35erez0JZbzzyi21kYF1LSj4hokAT9iIgGSdCPiGiQBP2IiAZJ0I+IaJB2FkbfW9K1LT/3STpF0s6SVkq6uT7u1HLO6ZLWSLpJ0qEt6QdIWl33nVWXTYyIiA5pZ7nEm4D9ASRNAu4AvgosAVbZPlPSkvr7aZL2ARYA+wJPBr4j6el1ycRzgMXA94BvAIeRJRPblq5zEfF4Dbd652Dgp7ZvA+YBy2r6MuCouj0PuND2A7ZvAdYAcyXtDkyxfYVtA+e3nBMRER0w3KC/ALigbu9m+06A+jitpk8H1racs66mTa/b/dM3IWmxpB5JPb29vcPMYkREDKbtoC9pK+A1wL8OdegAad5M+qaJ9rm259ieM3Xq1HazGBERQxhOSf9w4Brbd9Xf76pVNtTHDTV9HTCz5bwZwPqaPmOA9IiI6JDhBP1j+H3VDsAKYFHdXgRc1JK+QNLWkvYCZgNX1SqgjZIOqr12FracExERHdDWhGuStgVeAfxxS/KZwHJJxwO3A/MBbF8vaTlwA/AQcFLtuQNwInAeMJnSayc9dyIiOqitoG/7fmCXfml3U3rzDHT8UmDpAOk9wH7Dz2ZERIyEjMiNiGiQBP2IiAZJ0I+IaJAE/YiIBknQj4hokAT9iIgGSdCPiGiQBP2IiAZJ0I+IaJAE/YiIBknQj4hokAT9iIgGaWvCtYjRknV/IzorJf2IiAZJ0I+IaJC2gr6kHSV9WdKPJd0o6XmSdpa0UtLN9XGnluNPl7RG0k2SDm1JP0DS6rrvrLqCVkREdEi7Jf1PAt+0/QzgWcCNwBJgle3ZwKr6O5L2ARYA+wKHAWdLmlSf5xxgMWUJxdl1f0REdMiQQV/SFODFwGcBbP+v7XuBecCyetgy4Ki6PQ+40PYDtm8B1gBz6+LpU2xfYdvA+S3nREREB7RT0n8K0Av8s6QfSPqMpO2A3epi59THafX46cDalvPX1bTpdbt/+iYkLZbUI6mnt7d3WBcUERGDayfobwE8BzjH9rOB31CrcgYxUD29N5O+aaJ9ru05tudMnTq1jSxGREQ72gn664B1tq+sv3+ZchO4q1bZUB83tBw/s+X8GcD6mj5jgPSIiOiQIYO+7Z8DayXtXZMOBm4AVgCLatoi4KK6vQJYIGlrSXtRGmyvqlVAGyUdVHvtLGw5JyIiOqDdEbnvBL4gaSvgZ8BbKDeM5ZKOB24H5gPYvl7ScsqN4SHgJNsP1+c5ETgPmAxcUn8iIqJD2gr6tq8F5gyw6+BBjl8KLB0gvQfYbxj5i4iIEZQRuRERDZKgHxHRIAn6ERENkqAfEdEgCfoREQ2SoB8R0SAJ+hERDZKgHxHRIAn6ERENkqAfEdEgCfoREQ2SoB8R0SAJ+hERDZKgHxHRIAn6ERENkqAfEdEgbQV9SbdKWi3pWkk9NW1nSSsl3Vwfd2o5/nRJayTdJOnQlvQD6vOskXRWXTYxIiI6ZDgl/ZfZ3t923wpaS4BVtmcDq+rvSNoHWADsCxwGnC1pUj3nHGAxZd3c2XV/RER0yOOp3pkHLKvby4CjWtIvtP2A7VuANcBcSbsDU2xfYdvA+S3nREREB7Qb9A18W9LVkhbXtN1s3wlQH6fV9OnA2pZz19W06XW7f/omJC2W1COpp7e3t80sRkTEUNpaGB14ge31kqYBKyX9eDPHDlRP782kb5ponwucCzBnzpwBj4mIiOFrq6Rve3193AB8FZgL3FWrbKiPG+rh64CZLafPANbX9BkDpEdERIcMGfQlbSdph75t4JXAj4AVwKJ62CLgorq9AlggaWtJe1EabK+qVUAbJR1Ue+0sbDknIiI6oJ3qnd2Ar9belVsAX7T9TUnfB5ZLOh64HZgPYPt6ScuBG4CHgJNsP1yf60TgPGAycEn9iYiIDhky6Nv+GfCsAdLvBg4e5JylwNIB0nuA/YafzYiIGAkZkRsR0SAJ+hERDZKgHxHRIAn6ERENkqAfEdEgCfoREQ2SoB8R0SAJ+hERDZKgHxHRIAn6ERENkqAfEdEgCfoREQ2SoB8R0SAJ+hERDZKgHxHRIG0HfUmTJP1A0sX1950lrZR0c33cqeXY0yWtkXSTpENb0g+QtLruO6uuoBURER0ynJL+u4EbW35fAqyyPRtYVX9H0j7AAmBf4DDgbEmT6jnnAIspSyjOrvsjIqJD2gr6kmYARwCfaUmeByyr28uAo1rSL7T9gO1bgDXA3Lp4+hTbV9g2cH7LORER0QHtlvT/Dng/8EhL2m51sXPq47SaPh1Y23Lcupo2vW73T4+IiA4ZMuhLOhLYYPvqNp9zoHp6byZ9oNdcLKlHUk9vb2+bLxsREUNpp6T/AuA1km4FLgReLunzwF21yob6uKEevw6Y2XL+DGB9TZ8xQPombJ9re47tOVOnTh3G5URExOYMGfRtn257hu1ZlAba/7B9LLACWFQPWwRcVLdXAAskbS1pL0qD7VW1CmijpINqr52FLedEREQHbPE4zj0TWC7peOB2YD6A7eslLQduAB4CTrL9cD3nROA8YDJwSf2JiIgOGVbQt30pcGndvhs4eJDjlgJLB0jvAfYbbiYjImJkZERuRESDJOhHRDRIgn5ERIMk6EdENEiCfkREgyToR0Q0SIJ+RESDJOhHRDRIgn5ERIMk6EdENEiCfkREgyToR0Q0SIJ+RESDJOhHRDRIgn5ERIMk6EdENEg7C6NvI+kqST+UdL2kD9f0nSWtlHRzfdyp5ZzTJa2RdJOkQ1vSD5C0uu47qy6bGBERHdJOSf8B4OW2nwXsDxwm6SBgCbDK9mxgVf0dSftQ1tLdFzgMOFvSpPpc5wCLKevmzq77IyKiQ9pZGN22f11/3bL+GJgHLKvpy4Cj6vY84ELbD9i+BVgDzJW0OzDF9hW2DZzfck5ERHRAW3X6kiZJuhbYAKy0fSWwm+07AerjtHr4dGBty+nratr0ut0/faDXWyypR1JPb2/vMC4nIiI2p62gb/th2/sDMyil9s0tbj5QPb03kz7Q651re47tOVOnTm0nixER0YZh9d6xfS9wKaUu/q5aZUN93FAPWwfMbDltBrC+ps8YID0iIjqknd47UyXtWLcnA4cAPwZWAIvqYYuAi+r2CmCBpK0l7UVpsL2qVgFtlHRQ7bWzsOWciIjogC3aOGZ3YFntgfMEYLntiyVdASyXdDxwOzAfwPb1kpYDNwAPASfZfrg+14nAecBk4JL6ExERHTJk0Ld9HfDsAdLvBg4e5JylwNIB0nuAzbUHRETEKMqI3IiIBknQj4hokAT9iIgGSdCPiGiQBP2IiAZJ0I+IaJAE/YiIBknQj4hokAT9iIgGSdCPiGiQBP2IiAZpZ8K1iGjTrCVf73YW2nLrmUd0OwvRJSnpR0Q0SIJ+RESDJOhHRDRIOytnzZT0XUk3Srpe0rtr+s6SVkq6uT7u1HLO6ZLWSLpJ0qEt6QdIWl33nVVX0IqIiA5ppyH3IeBU29dI2gG4WtJK4Dhgle0zJS0BlgCnSdoHWADsCzwZ+I6kp9fVs84BFgPfA75BWWt31FbPSqNaRMT/NWRJ3/adtq+p2xuBG4HpwDxgWT1sGXBU3Z4HXGj7Adu3AGuAuXXx9Cm2r7Bt4PyWcyIiogOGVacvaRZl6cQrgd3qYufUx2n1sOnA2pbT1tW06XW7f3pERHRI20Ff0vbAvwGn2L5vc4cOkObNpA/0Wosl9Ujq6e3tbTeLERExhLaCvqQtKQH/C7a/UpPvqlU21McNNX0dMLPl9BnA+po+Y4D0Tdg+1/Yc23OmTp3a7rVERMQQ2um9I+CzwI22P96yawWwqG4vAi5qSV8gaWtJewGzgatqFdBGSQfV51zYck5ERHRAO713XgC8GVgt6dqa9gHgTGC5pOOB24H5ALavl7QcuIHS8+ek2nMH4ETgPGAypdfOqPXciYiITQ0Z9G1fxsD18QAHD3LOUmDpAOk9wH7DyWBERIycjMiNiGiQBP2IiAZJ0I+IaJAE/YiIBknQj4hokAT9iIgGSdCPiGiQBP2IiAZJ0I+IaJAE/YiIBknQj4hokAT9iIgGSdCPiGiQBP2IiAZJ0I+IaJAE/YiIBmlnucTPSdog6UctaTtLWinp5vq4U8u+0yWtkXSTpENb0g+QtLruO6sumRgRER3UTkn/POCwfmlLgFW2ZwOr6u9I2gdYAOxbzzlb0qR6zjnAYsqaubMHeM6IiBhlQwZ92/8F3NMveR6wrG4vA45qSb/Q9gO2bwHWAHMl7Q5MsX2FbQPnt5wTEREd8ljr9HezfSdAfZxW06cDa1uOW1fTptft/ukDkrRYUo+knt7e3seYxYiI6G+kG3IHqqf3ZtIHZPtc23Nsz5k6deqIZS4ioukea9C/q1bZUB831PR1wMyW42YA62v6jAHSIyKigx5r0F8BLKrbi4CLWtIXSNpa0l6UBturahXQRkkH1V47C1vOiYiIDtliqAMkXQC8FNhV0jrgQ8CZwHJJxwO3A/MBbF8vaTlwA/AQcJLth+tTnUjpCTQZuKT+REREBw0Z9G0fM8iugwc5fimwdID0HmC/YeUuIiJGVEbkRkQ0SIJ+RESDJOhHRDRIgn5ERIMk6EdENEiCfkREgyToR0Q0SIJ+RESDJOhHRDRIgn5ERIMk6EdENEiCfkREgyToR0Q0SIJ+RESDJOhHRDRIgn5ERIN0POhLOkzSTZLWSFrS6dePiGiyjgZ9SZOATwGHA/sAx0jap5N5iIhosk6X9OcCa2z/zPb/AhcC8zqch4iIxpLtzr2Y9DrgMNtvq7+/GTjQ9sn9jlsMLK6/7g3c1LFMDm1X4BfdzsQImmjXAxPvmiba9cDEu6axeD172p7aP3HIhdFHmAZI2+SuY/tc4NzRz87wSeqxPafb+RgpE+16YOJd00S7Hph41zSerqfT1TvrgJktv88A1nc4DxERjdXpoP99YLakvSRtBSwAVnQ4DxERjdXR6h3bD0k6GfgWMAn4nO3rO5mHETAmq50eh4l2PTDxrmmiXQ9MvGsaN9fT0YbciIjorozIjYhokAT9iIgGSdCPmCDqiPeIzUrQjxElacuW7a69vyQNNCZkQpL0fEmTbD+cwD82SPoDSc/qdj4GkqAfI0bSDsDxknaU9Brg7V3Kh1x7KEjavRt56LB3Aj+R9ITxEvgHuil3s5Awkurn4I+Ad0v6w27np78J8UceK/reyJKeLWnfsfgPH022NwK/BX4CLAU+16V89AX8U4ALJE3pRj5GW19wt30M8GPgP8dD4Je0bcv/6AWSXgxg+5HxHvhrgWMj8AXgduBESU/vcrb+j3H9Bx5L6ofNkg4HzgdeDvyHpHExNPvx6FdquwHopUyvsWPdP2mQY0czT2+lDP471vZ9knbuxOt2ku2HASQdA1wL7AlcMZYDfy0InShpV0knUD4r75d0JUyMwF8dDDwDeBHwHknP7HJ+HjUR/rhdJWk7ePTNOgM4HXgNcCewlnK3n7D6VaXsYPv7tvcF/gm4RNI+NQDtL2lLd25gyI7AXwH7SDoVuFLS+yU9cSLV90s6BPgL4NPAgcAaoKcl8I+1z/h2wEHAHwPPA55r+0hg3XgO/JK2gPItU9LLgfcAJ1DiwVrghLFS4h9Xf9ixRtJOwBJJR9ekXuBS4IXAqcAC2xskHSVp5iBPM271C/jvBf5N0ipJ02z/PbC8pn0Q+Ci15D/KeTpS0h6UmVnfAbyX8qFbQgk2T+zgjWfEDXDDuhe4xPZa23fafhPwCPDDGvgf6XgmB9AXxG1/j3KT+gNgFvDUmv5HwO2SflJ/HxP5boek5wB/KmmXmrQd8BPb99q+GPgOZf2QPxsL64ck6D8+Ah4GnifpSNsPAHOAzwAvs/0TSXOB04AdupjPUdES8A8DjgDeCtxMqUd/qu2PUoL9M4BTbPeOZn4kbU35On0u8D/A0cAf2V4O/Ap4EvDgaOZhNPW7yW5Tr/dO4EhJL2k5dBlwD7BHF7K5iZrvR+r2fMr/4iOUm/HzJM0GsD2f8o1sr65l9rG5G3gppTS/A3A5sL2khQC2rwJ+RPnWf0+3Mtkn0zA8Ri1d5HYBjqOUWr4ErAa+AfwUuA54E3CG7Yu6lNVRJem5lFL0Gtun1bSPAfsDJ9m+SdJWddGcTuRnV+Bk4Ln19W+V9A7gbcAi26s7kY/RJOldlG8tWwJnA1tRGg4/BOxCqeZZaPvurmVyALVh/fXA22zfIOlA4CTgauBbtn/czfw9Fn034tp29xfAVZTVAV8GvBL4DfADyvogr7N9R9cyW6Wk/xjVgP984BTgLOBWyht6f0ojznWUf/h7bV80UeqRB7iOn1N66zxd0ssAbL+PUuL/W5V++yNeum6t85X0Bkkfqa/9C8r/45r6+rsBV1A+cBMh4L+F0mZ0CiXon2z7W5RG652A3YHTxmDA3wt4LXBkDfhPsH0l8A+UUvLBahnjMdb1fQ5qwH8r8Arg3ZTCxlsowf/jlBvys4HFYyHgQ0r6j0vtEXIxpbRyK6Vf+pModawru5i1UdGvemEBpdDQC/w3pcFqa+Cbti+tx0yzvWGU8zSNsmrRt4CP1rYEanXHJyi9iRaOpzrizZF0PPBdSuA/lLLcqCltFWNm5abW90r9/RnABcArbP+i5ZvyVpRqqPttj7u1NSQdS2mfWFarc2cB5wCXAZ+wff9YaluBlPSHpe/u3tIodQ9lPYDn2P4lpS71l5Q61t26ltFR0hLwT6QMCLqPEmyfTbn2+4HXSXpRPWXE6/BVRp8uqNsnASuBRZSS1f+raQDTgEuA94+lD9xwDPLtcColoDzf9uG12ux4ykCgrcbCN8p+hYOZtQT/U8p7Zb6knWvAXwR8FrhtvAT8/jGA0o51MuVzj+1bKb12Dgfe1dqeMVakpN+G2tXwwbo9l9I4eTbwQ2Ab4MvAIbZvkzQV2MH2z7qW4VFS3/C7AX9P+VbzBuB1wOF1rYQnAwsp6ySMSglf0hGUKoHzKe0oHwZmA08BjqFUcVwDvLjma9zVE/dXqw92oNzYrgIupKyFsZDSZnQyMN/2jV3L5ABqj64XUXoYXUbpVbQPMBdYRcn7UWMt34NpLbFL2tH2vXX7a5RvWi9uOXYmgO213cjr5iToD6GWUl5MGWkKZbnHJ1G+Ur+N0nh2AvAvtj/flUyOov5fTSVNBv6cUpUzC3iD7d9Keg/lW88to12ykfQKStXND22/qfZieQqlLnUlpY3hwfFSeuyvX0n5cMp4g+8CUyg3tGWUKgRRGm7fZ/uGLmV3QJJeCXzQ9kskXQb02D5F0p6ULs1bApfbvrmrGX0MJC2m1OHfA1xs+2uS/h3YzvYrupq5NiToD0HSNpRBJGcATwde2dcgWFvsX0WpV/1f28/rVj5Hm8pIyjtrfexfU3rGbF/3vZ7SLfVo27d1KD/zgPOAE2x/qab9O+Xm+2+dyMNo6Bfw96S8ty6zfY2kIyldY39g+9x6zLa27+9ejovatjLV9vW1PeUgSiP/NpSus/Ns/07SXrZv6WZeHw9Jrwb+hvIt5WDgycANts+VdBWw3vZRXczikDq6XOJ4Uz+Av5P0U2AvSilrBqVbJrZ7gB7gzyVdLOmNtr/YvRyPDpUlLk8Dvi3pDtunSZol6ZvABmBv4LhOBXyA2iPqzcBZKgNerqI0CF7bqTyMhpaAfwqlN9gelIbqayjfYgwskLSN7bP4/TfQbnsi8AlJvcC2lOq3JZQG2pfBo9U9z5B0Ul916XhSe+vtBXzS9tWSbqRMt/JGSZ+1PbfeqMe0BP1B9JW4JB1KaaQ5kFIXebSkXWx/XmUGx61qsFtNaWQb9/qVNnenBJ0XA9sDJ0s60/YbJO1PqXK41XbHp5uwfbHK8Pd/A/6V8k3j1k7nY6TV99wLKQHlRcA5kq63/SVJq4CHKO1Jj94kus32zZKuo/RHf7/tr9ZvJnfUb4LbAm8G3jxeAn6/z8EUSul+R+Bpki63fR1wscq4iecA3+9kweexStAfRA34r6HMFvkB21dJ+m9KQ+FLVAYlPRtYLGlHSm+RL3QtwyOk3xv9LcDzKW0Yn6KMKPw7Sk+Rz1GqVjoy6Gowtv9dZa6TW8fDB24gA9xkj6G0Udj2SknvppSit7G9jNILZiz6NOVm9F5J64A/AQ4BjqX03llo+0ddzN+wtPxP9nEZW3ANpaT/fcokahcAkynfcsZcg+1gUqc/iBrIv0JppL2ZciefRRnevw/lrv8Vl7k1xkzd6kiR9FpKr5BllK/pf0fpi/xArd8/DviY7Tu7lskJoF/Afznl/TWX0jvqp8DHXWYJfS2l08CLgF+PlRL+QFoKS6dQuoW/kDKG4jfdzNdjIel5lN5SfwX8B2WKlSsogf8EYCPwYds/7FomhylBfxAqk6l9mTKB2t6U7mavBD5i+5Mtx01yneJ2opB0AKVO9lTb31SZ7/wMygRq59V2jo5NrdAEKmMf3kUpGd9Jqdo5gtLd8ZO275W03XgJnCrzMX2UUhX1xvHYdVZl4Ng0yvt+G0qvtRdQBsUdTbkpb+Uy59a4kcFZVe2DjqQ5kp5N6Rf9Pko99mdtL6SUbp8jabJ+P0Br3Ad8bTqN7S8p3R5Pl7Sr7f8C/h+lvvZNAAn4I6feZI+n9Ay7o3Z5XUUZ7T2DMv+8GDuNtkOy/U1K75ZDx2nAfz7wQUrVzZuAdZT6/JuB/YA3Ak8YbwEfEvQfXeCj1uEfDvwzpYtmX0PZaba/W/uGfwy40PZvR7sveqfU6oW+ASdzJc11GVh2HGWiqE/UwH85Zari73QvtxNDSwGjb/TsJODHttdKeoKkLWr1zWWU1cc+52Jcveds93qUZ1YdRWvrzzLK3EBfB+6rXWXfDnxhvBb4Gl29I2lv4DDKB2tbyiyZxwEHAB+g9MHfQGm8/RfgbNtf70pmR0G/+uR3UiaMuoXyvjhEZcbK0ylznr/VZdqJeBz6/c0nuwxs25FyM/207c/UfScAT7J9RtcyG6gsbn4m5Zv/rraf0eUsPW6NDfoqUyJ/izJ3xo8p/9RTgJ9RRtq+xWUCpaMpDTgP297Y+qEdz1rrh2tj1cmUGRt/KenblK+uh6gMunkP8PcepyNcxyKVidMOp7zfrgZuo1ShraHceI+lvAfH/cyg4139DBxMKRQtGO/dgpse9C+mfIXeg7J82+cpjbU72/61yjw7n6J8+MZNV7OhSHoa8EeUqQymUKq0dqSMsr2uHvNNYBfbz9UYmyVwvJP0Rso3qPdQ5g16PmWRjQspgeVu4KseY1MrNJ1a5uAazxoX9CU9Bdje9nWSTqf0SvlH2++StC1l1OMdlBGQb2ACLoCislJRL+Vmdw+lsfrPKDfAS1yHyatMa/Cubgy8mkgGGeTzK9tflLQ9pTvw2ynjHsZF75wYv5rYkDsXmFwbcC+n1N0/T2Vo+P2UftA9lG5zp3gCLoDiMsmVKVUIf0ZZvu6vKSXOV9VvAtg+KgH/8ekX8N9BmRRuCnBabSD/NaV6ZyfKPC4Ro6oxJX2VlXvusf0rSU8Cvg0cb/v7kg6h9Mw5x/Y/djWjo2Sgtoga3I+lTLOwlDKNxF9Sqr0+Z/uhjmd0gpL0x5Ruma+1fYekPwf2pVTxHEhpT3pdBrvFaGtS0D+EMj/LXnWgy59Qlph7h+0rJb0U+Cfgb21/uns5HXn9Sptvp8yJfw9lANZOlP73UyiDaXai3BzTaDtCVKajvoAyHXIPZSK16ZSbwOWUTgR/0teeEjGaGhP04dFRgp8C5tReKu+k9NR5u8vcOgdTpkj+765mdJSozHn/auCTlIFnP6OUMCcDp1JGHS8Zr/2PxzKVOdhPoAzyuYnSW2cmpTvgb23/rovZiwZpVNCHRxel+Ad+H/hPonzFPtb297qbu5HVOkWEytqd7wPeSwn0L6eMut2ZssbvtsBD43gwzZimsi7DHwI/tX2PpDdRChxHeALN2RRjX+OCPvyfwH9Arep5N3DNRCrh1wE/s2ubxYsoUyv8grIQzFLgZZSRhn2LOL91Iow/GOvqlBdvodx4j5lIXYFjfGjk1Mq2L6kl/JskPaN1ArUJZDpwsKQlwB8Az/bvZ8i82mVN22mUuuZPJ+B3zDaUarTXe5ysDRsTSyNL+n1UFtn+je1Lu52XkdKv0fZcSh3+p2z/ZU17GmUekSsoowwPsX1Tt/LbRBNlVHeMT40O+n0myoewX8BfSFm8+XuUmRrXUyaL65X0TEod/vr0w49olgT9CahOH3EqZaWiB+qw/wMpQ/13pHQR/EtneuSIxmlknf5E01fCryNuZ1Im7tqV0mi7ug73fwh4FmVhjjcn4Ec0U0r641y/Kp0n2H5E0j7Ah6kLcdheV/dvAWxr+77u5TgiuilBf4KQ9C6gr67+o5Rvce8G/hv4ZuruIwKaOeHahFNHe76GsnD27sB7bPdQpkw+Ejikb4WwiGi21OmPQ9p0MfZtgTdT5hL6LfB2lUWd/xP4NXBHplaICEjQH3ck7UTpgrla0quAa4E9ge8CP7D9qnrcScB2tj/arbxGxNiToD/+zACOlbQ7cBCwN2VO/JdQ1vPtW4rvHZTVsSIiHpWgP87YXi3pAeBo4IO1587GOrp4uaRllG6b823/uJt5jYixJ713xoH+I4Yl7QG8gDKNwn9ReudskPREYCOlWmdjd3IbEWNZSvpj3ADL7T2VUo//BeBeyspXv5W0NzANeG8CfkQMJkF/jGsJ+C+lBPgVlCkVngN8kLLW7Sso9fvvyBKHEbE5qd4Zo/qV8N9EmUvnrbavlTQHmA9sSZlD5x5J29dFtiMiBpXBWWNQv4C/E3AlZRHtBQB14NVyStBfUvvtJ+BHxJBSvTMGtQT8kykjar9OWbT9nZLus/1Xtq+W9DAZeBURw5CgP0ZJOgp4PTAP+BpwMaUe/7uStrb9IdvXdi+HETEepU5/jJK0iNL9cgrwRuAo2/fXm8HHKI2590yExV8ionNS0h+7bgU+R1nd6kUAkk4FHgb2Tx1+RDwWCfpj19XARcAjtbvmHpQS/6IE/Ih4rFK9M4bV+XVeU3/uBv7G9uru5ioixrME/XFA0pYAth/sdl4iYnxL0I+IaJAMzoqIaJAE/YiIBknQj4hokAT9iIgGSdCPiGiQBP2IiAZJ0I+IaJD/D37MQqMWzRVeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4\n",
    "prefix_txt = 'data_channel_is_'\n",
    "names = ['lifestyle', 'entertainment', 'bus', 'socmed', 'tech', 'world']\n",
    "# Calculate count of each channels with sum function\n",
    "count_channels = [df[prefix_txt + names[i]].sum() for i in range(len(names))]\n",
    "print(count_channels)\n",
    "# plot count of each channel\n",
    "plt.title('Data channels count bar chart')\n",
    "plt.xticks(rotation=45)\n",
    "plt.bar(names, count_channels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dataset Analysis</h1>\n",
    "<h2>1</h2>\n",
    "after remove bad rows, dataset total number of samples is 39634.\n",
    "<h2>2</h2>\n",
    "visualize first 15 samples of dataset.\n",
    "<h2>3</h2>\n",
    "plot Distribution of shares histogram.\n",
    "we have data with very high shares like numbers bigger than 800000, but most of them are between 0 and 100000 shares.\n",
    "<h2>4</h2>\n",
    "A bar chart counting the attributes: data_channel_is_lifestyle, data_channel_is_entertainment, data_channel_is_bus, data_channel_is_socmed, data_channel_is_tech, data_channel_is_world\n",
    "in last part i remove and handled all bool columns, then we have clean data, then we can sum columns value to get number of each attribute.\n",
    "then we use sum function of dataframe series and get number of occurence of each types.\n",
    "then visualize them with bar plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Feature importance analysis  (up to 1 of 11.2 points)\n",
    "\n",
    "Perform feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.025844</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.000702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.030092</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.009868</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.881250</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.009868</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.062662</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.029605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.630303</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.126505</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.163793</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.779808</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.000598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39629</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.192235</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.049342</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.570466</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.001659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39630</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.016447</td>\n",
       "      <td>0.025862</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39631</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.040831</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.029605</td>\n",
       "      <td>0.060345</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39632</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.038707</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.029605</td>\n",
       "      <td>0.060345</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.527473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39633</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.080481</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.032895</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.794754</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39634 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       timedelta  n_tokens_title  n_tokens_content  n_unique_tokens  \\\n",
       "0            1.0        0.476190          0.025844         0.000947   \n",
       "1            1.0        0.333333          0.030092         0.000863   \n",
       "2            1.0        0.333333          0.024900         0.000820   \n",
       "3            1.0        0.333333          0.062662         0.000719   \n",
       "4            1.0        0.523810          0.126505         0.000593   \n",
       "...          ...             ...               ...              ...   \n",
       "39629        0.0        0.523810          0.192235         0.000607   \n",
       "39630        0.0        0.428571          0.026316         0.000932   \n",
       "39631        0.0        0.428571          0.040831         0.000755   \n",
       "39632        0.0        0.476190          0.038707         0.000993   \n",
       "39633        0.0        0.190476          0.080481         0.000770   \n",
       "\n",
       "       n_non_stop_words  n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  \\\n",
       "0               0.00096                  0.001254   0.013158        0.017241   \n",
       "1               0.00096                  0.001218   0.009868        0.008621   \n",
       "2               0.00096                  0.001021   0.009868        0.008621   \n",
       "3               0.00096                  0.001024   0.029605        0.000000   \n",
       "4               0.00096                  0.000832   0.062500        0.163793   \n",
       "...                 ...                       ...        ...             ...   \n",
       "39629           0.00096                  0.000932   0.049342        0.103448   \n",
       "39630           0.00096                  0.001270   0.016447        0.025862   \n",
       "39631           0.00096                  0.001054   0.029605        0.060345   \n",
       "39632           0.00096                  0.001362   0.029605        0.060345   \n",
       "39633           0.00096                  0.001066   0.032895        0.008621   \n",
       "\n",
       "       num_imgs  num_videos  ...  min_positive_polarity  \\\n",
       "0      0.007812    0.000000  ...               0.100000   \n",
       "1      0.007812    0.000000  ...               0.033333   \n",
       "2      0.007812    0.000000  ...               0.100000   \n",
       "3      0.007812    0.000000  ...               0.136364   \n",
       "4      0.156250    0.000000  ...               0.033333   \n",
       "...         ...         ...  ...                    ...   \n",
       "39629  0.046875    0.000000  ...               0.033333   \n",
       "39630  0.007812    0.000000  ...               0.214286   \n",
       "39631  0.007812    0.010989  ...               0.100000   \n",
       "39632  0.023438    0.527473  ...               0.136364   \n",
       "39633  0.007812    0.000000  ...               0.062500   \n",
       "\n",
       "       max_positive_polarity  avg_negative_polarity  min_negative_polarity  \\\n",
       "0                       0.70               0.650000                  0.400   \n",
       "1                       0.70               0.881250                  0.875   \n",
       "2                       1.00               0.533333                  0.200   \n",
       "3                       0.80               0.630303                  0.400   \n",
       "4                       1.00               0.779808                  0.500   \n",
       "...                      ...                    ...                    ...   \n",
       "39629                   1.00               0.570466                  0.000   \n",
       "39630                   0.80               0.750000                  0.750   \n",
       "39631                   0.75               0.740000                  0.500   \n",
       "39632                   0.70               0.788889                  0.600   \n",
       "39633                   0.50               0.794754                  0.500   \n",
       "\n",
       "       max_negative_polarity  title_subjectivity  title_sentiment_polarity  \\\n",
       "0                   0.800000            0.500000                  0.406250   \n",
       "1                   0.900000            0.000000                  0.500000   \n",
       "2                   0.866667            0.000000                  0.500000   \n",
       "3                   0.833333            0.000000                  0.500000   \n",
       "4                   0.950000            0.454545                  0.568182   \n",
       "...                      ...                 ...                       ...   \n",
       "39629               0.950000            0.783333                  0.200000   \n",
       "39630               0.750000            0.000000                  0.500000   \n",
       "39631               0.875000            0.100000                  0.500000   \n",
       "39632               0.900000            0.300000                  1.000000   \n",
       "39633               0.987500            0.000000                  0.500000   \n",
       "\n",
       "       abs_title_subjectivity  abs_title_sentiment_polarity    shares  \n",
       "0                    0.000000                      0.187500  0.000702  \n",
       "1                    1.000000                      0.000000  0.000842  \n",
       "2                    1.000000                      0.000000  0.001778  \n",
       "3                    1.000000                      0.000000  0.001422  \n",
       "4                    0.090909                      0.136364  0.000598  \n",
       "...                       ...                           ...       ...  \n",
       "39629                0.566667                      0.600000  0.001659  \n",
       "39630                1.000000                      0.000000  0.001422  \n",
       "39631                0.800000                      0.000000  0.002133  \n",
       "39632                0.400000                      1.000000  0.002252  \n",
       "39633                1.000000                      0.000000  0.001303  \n",
       "\n",
       "[39634 rows x 60 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Normalize data with min-max\n",
    "df_model = df.iloc[:, 1:] # Remove url column for train\n",
    "scaler = MinMaxScaler()\n",
    "df_model[df_model.columns] = scaler.fit_transform(df_model[df_model.columns])\n",
    "display(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def a function that convert a dataframe to X and y (data and label)\n",
    "def df2numpy(dataframe):\n",
    "    return dataframe.iloc[:, :-1].to_numpy(), dataframe.iloc[:, -1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def a function to get importancy of each feature (with any model send to this func.)\n",
    "def get_feature_importance(X, y, model):\n",
    "    # fit the model\n",
    "    model.fit(X, y)\n",
    "    # get importance\n",
    "    importance = model.coef_\n",
    "\n",
    "    # summarize feature importance\n",
    "    for i,v in enumerate(importance):\n",
    "        print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "    # plot feature importance\n",
    "    plt.bar([x for x in range(len(importance))], importance)\n",
    "    plt.show()\n",
    "    return importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.00144\n",
      "Feature: 1, Score: 0.00280\n",
      "Feature: 2, Score: 0.00592\n",
      "Feature: 3, Score: 2.78429\n",
      "Feature: 4, Score: -1.96097\n",
      "Feature: 5, Score: -1.05789\n",
      "Feature: 6, Score: 0.00945\n",
      "Feature: 7, Score: -0.00847\n",
      "Feature: 8, Score: 0.00174\n",
      "Feature: 9, Score: 0.00044\n",
      "Feature: 10, Score: -0.00518\n",
      "Feature: 11, Score: 0.00059\n",
      "Feature: 12, Score: -0.00114\n",
      "Feature: 13, Score: -0.00128\n",
      "Feature: 14, Score: -0.00092\n",
      "Feature: 15, Score: -0.00062\n",
      "Feature: 16, Score: -0.00057\n",
      "Feature: 17, Score: -0.00037\n",
      "Feature: 18, Score: 0.00071\n",
      "Feature: 19, Score: 0.03819\n",
      "Feature: 20, Score: -0.02507\n",
      "Feature: 21, Score: -0.00249\n",
      "Feature: 22, Score: -0.00003\n",
      "Feature: 23, Score: 0.00005\n",
      "Feature: 24, Score: -0.00156\n",
      "Feature: 25, Score: -0.07292\n",
      "Feature: 26, Score: 0.08704\n",
      "Feature: 27, Score: 0.02659\n",
      "Feature: 28, Score: 0.00588\n",
      "Feature: 29, Score: -0.00643\n",
      "Feature: 30, Score: 658343550.07328\n",
      "Feature: 31, Score: 658343550.07264\n",
      "Feature: 32, Score: 658343550.07283\n",
      "Feature: 33, Score: 658343550.07262\n",
      "Feature: 34, Score: 658343550.07267\n",
      "Feature: 35, Score: 139193286.73949\n",
      "Feature: 36, Score: 139193286.73903\n",
      "Feature: 37, Score: 519150263.33393\n",
      "Feature: 38, Score: -0.22321\n",
      "Feature: 39, Score: -0.22383\n",
      "Feature: 40, Score: -0.22289\n",
      "Feature: 41, Score: -0.22356\n",
      "Feature: 42, Score: -0.22371\n",
      "Feature: 43, Score: 0.00297\n",
      "Feature: 44, Score: 0.00108\n",
      "Feature: 45, Score: -0.00257\n",
      "Feature: 46, Score: 0.00002\n",
      "Feature: 47, Score: 0.00240\n",
      "Feature: 48, Score: 0.00251\n",
      "Feature: 49, Score: -0.00200\n",
      "Feature: 50, Score: -0.00225\n",
      "Feature: 51, Score: 0.00037\n",
      "Feature: 52, Score: -0.00202\n",
      "Feature: 53, Score: 0.00010\n",
      "Feature: 54, Score: -0.00021\n",
      "Feature: 55, Score: -0.00011\n",
      "Feature: 56, Score: 0.00048\n",
      "Feature: 57, Score: 0.00039\n",
      "Feature: 58, Score: 0.00074\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEDCAYAAAAcI05xAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAND0lEQVR4nO3db4xldX3H8fdHFovyp2h3NESgUxuDElJ2yQSkNEYXarZg7BObQKKxDc2miRpMTMiSNm3tI57U6IOWZItIUynEorQGUoTwJ9TEYmdhoQsLxeK2bEB3iCGoD6TAtw/uWZhdB+bu7tyZ7519v5LJvefck7vfX2DfHM6cO5OqQpLU11vWegBJ0psz1JLUnKGWpOYMtSQ1Z6glqTlDLUnNTSzUSW5Isj/J7jGOPTPJfUkeTvJokksnNZckTZtJnlHfCGwd89g/A75RVZuBy4G/ndRQkjRtJhbqqnoA+MnifUl+M8mdSXYm+bck7z9wOHDK8PxXgWcnNZckTZsNq/zn7QD+pKqeSnIBozPnLcBfAncl+RxwInDJKs8lSW2tWqiTnAT8NvBPSQ7s/pXh8Qrgxqr66yQXAv+Q5JyqenW15pOkrlbzjPotwAtVtWmJ165kuJ5dVd9LcgKwEdi/euNJUk+rdnteVb0I/DDJHwBk5Nzh5f8FLh72fwA4AVhYrdkkqbNM6qfnJbkZ+DCjM+MfA38B3AtcB5wGHA/cUlV/leRs4O+Akxh9Y/HqqrprIoNJ0pSZWKglSSvDTyZKUnMT+Wbixo0ba3Z2dhJvLUnr0s6dO5+vqpmlXptIqGdnZ5mfn5/EW0vSupTkf97oNS99SFJzhlqSmjPUktScoZak5gy1JDVnqCWpOUMtSc0ZaklqzlBLUnOr/RtepDZmt99x0Pbeay8ba9/hHLv32stWaFodyzyjlqTmDLUkNWeoJak5Qy1JzRlqSWrOUEtSc4Zakpoz1JLUnKGWpOYMtSQ1Z6glqTlDLUnNGWpJas5QS1JzY4U6yalJbk3yRJI9SS6c9GCSpJFxfx71V4A7q+oTSd4KvH2CM0mSFlk21ElOAT4E/CFAVb0EvDTZsSRJB4xz6eO9wALwtSQPJ7k+yYmHHpRkW5L5JPMLCwsrPqgkHavGCfUG4DzguqraDPwc2H7oQVW1o6rmqmpuZmZmhceUpGPXOKHeB+yrqgeH7VsZhVuStAqWDXVV/Qh4JslZw66LgccnOpUk6TXj3vXxOeCm4Y6Pp4E/mtxIkqTFxgp1Ve0C5iY7iiRpKX4yUZKaM9SS1JyhlqTmDLUkNWeoJak5Qy1JzRlqSWrOUEtSc4Zakpoz1JLUnKGWpOYMtSQ1Z6glqTlDLUnNGWpJas5QS1JzhlqSmjPUktScoZak5sb95baSJmx2+x0Hbe+99rI1mkTdeEYtSc0ZaklqbqxLH0n2Aj8FXgFerqq5SQ4lSXrd4Vyj/khVPT+xSSRJS/LShyQ1N26oC7gryc4k2yY5kCTpYONe+rioqp5N8i7g7iRPVNUDiw8YAr4N4Mwzz1zhMSXp2DXWGXVVPTs87gduA85f4pgdVTVXVXMzMzMrO6UkHcOWDXWSE5OcfOA58FFg96QHkySNjHPp493AbUkOHP+PVXXnRKeSJL1m2VBX1dPAuaswiyRpCd6eJ0nNGWpJas5QS1JzhlqSmjPUktScoZak5gy1JDVnqCWpOUMtSc0ZaklqzlBLUnOGWpKaM9SS1JyhlqTmDLUkNWeoJak5Qy1JzRlqSWrOUEtSc4Zakpoz1JLUnKGWpOYMtSQ1N3aokxyX5OEkt09yIEnSwQ7njPoqYM+kBpEkLW2sUCc5HbgMuH6y40iSDjXuGfWXgauBV9/ogCTbkswnmV9YWFiJ2SRJjBHqJB8D9lfVzjc7rqp2VNVcVc3NzMys2ICSdKwb54z6IuDjSfYCtwBbknx9olNJkl6zbKir6pqqOr2qZoHLgXur6pMTn0ySBHgftSS1t+FwDq6q+4H7JzKJJGlJnlFLUnOGWpKaM9SS1JyhlqTmDLUkNWeoJak5Qy1JzRlqSWrOUEtSc4Zakpoz1JLUnKGWpOYMtSQ1Z6glqTlDLUnNGWpJas5QS1JzhlqSmjPUktScoZak5gy1JDVnqCWpuWVDneSEJN9P8kiSx5J8cTUGkySNbBjjmF8AW6rqZ0mOB76b5F+r6t8nPJskiTFCXVUF/GzYPH74qkkOJUl63VjXqJMcl2QXsB+4u6oeXOKYbUnmk8wvLCys8JiSdOwaK9RV9UpVbQJOB85Pcs4Sx+yoqrmqmpuZmVnhMSXp2HVYd31U1QvA/cDWSQwjSfpl49z1MZPk1OH524BLgCcmPJckaTDOXR+nAX+f5DhGYf9GVd0+2bEkSQeMc9fHo8DmVZhFkrQEP5koSc0ZaklqzlBLUnOGWpKaM9SS1JyhlqTmDLUkNWeoJak5Qy1JzRlqSWrOUEtSc4Zakpoz1JLUnKGWpOYMtSQ1Z6glqTlDLUnNGWpJas5QS1JzhlqSmjPUktScoZak5gy1JDW3bKiTnJHkviR7kjyW5KrVGEySNLJhjGNeBr5QVQ8lORnYmeTuqnp8wrNJkhjjjLqqnquqh4bnPwX2AO+Z9GCSpJHDukadZBbYDDy4xGvbkswnmV9YWFih8SRJY4c6yUnAN4HPV9WLh75eVTuqaq6q5mZmZlZyRkk6po0V6iTHM4r0TVX1rcmOJElabJy7PgJ8FdhTVV+a/EiSpMXGOaO+CPgUsCXJruHr0gnPJUkaLHt7XlV9F8gqzCJJWoKfTJSk5gy1JDVnqCWpOUMtSc0ZaklqzlBLUnOGWpKaM9SS1JyhlqTmDLUkNWeoJak5Qy1JzRlqSWpunF9uK+kozG6/46DtvddetuQ+6Y14Ri1JzRlqSWrOUEtSc4Zakpoz1JLUnKGWpOYMtSQ1Z6glqbllQ53khiT7k+xejYEkSQcb54z6RmDrhOeQJL2BZUNdVQ8AP1mFWSRJS1ixa9RJtiWZTzK/sLCwUm8rSce8FQt1Ve2oqrmqmpuZmVmpt5WkY553fUhSc4Zakpob5/a8m4HvAWcl2ZfkysmPJUk6YNlfHFBVV6zGIJKkpXnpQ5KaM9SS1JyhlqTmDLUkNWeoJak5Qy1JzRlqSWrOUEtSc4Zakpoz1JLUnKGWpOYMtSQ1Z6glqTlDLUnNGWpJas5QS1JzhlqSmjPUktScoZak5gy1JDVnqCWpOUMtSc0ZaklqbqxQJ9ma5MkkP0iyfdJDSZJet2yokxwH/A3we8DZwBVJzp70YJKkkXHOqM8HflBVT1fVS8AtwO9PdixJ0gGpqjc/IPkEsLWq/njY/hRwQVV99pDjtgHbhs2zgCePcraNwPNH+R6drKf1rKe1gOvpbj2t583W8utVNbPUCxvGeOMsse+X6l5VO4AdY7zfWJLMV9XcSr3fWltP61lPawHX0916Ws+RrmWcSx/7gDMWbZ8OPHu4f5Ak6ciME+r/AN6X5DeSvBW4HPj2ZMeSJB2w7KWPqno5yWeB7wDHATdU1WMTn2wFL6M0sZ7Ws57WAq6nu/W0niNay7LfTJQkrS0/mShJzRlqSWquXain/ePqSW5Isj/J7kX73pnk7iRPDY/vWMsZD0eSM5Lcl2RPkseSXDXsn8o1JTkhyfeTPDKs54vD/qlcD4w+PZzk4SS3D9vTvJa9Sf4zya4k88O+aV7PqUluTfLE8HfowiNZT6tQr5OPq98IbD1k33bgnqp6H3DPsD0tXga+UFUfAD4IfGb4ZzKta/oFsKWqzgU2AVuTfJDpXQ/AVcCeRdvTvBaAj1TVpkX3G0/zer4C3FlV7wfOZfTP6fDXU1VtvoALge8s2r4GuGat5zqCdcwCuxdtPwmcNjw/DXhyrWc8irX9C/C762FNwNuBh4ALpnU9jD7XcA+wBbh92DeVaxnm3QtsPGTfVK4HOAX4IcNNG0eznlZn1MB7gGcWbe8b9k27d1fVcwDD47vWeJ4jkmQW2Aw8yBSvabhUsAvYD9xdVdO8ni8DVwOvLto3rWuB0aee70qyc/ixFDC963kvsAB8bbg0dX2SEzmC9XQL9VgfV9fqS3IS8E3g81X14lrPczSq6pWq2sTobPT8JOes8UhHJMnHgP1VtXOtZ1lBF1XVeYwuf34myYfWeqCjsAE4D7iuqjYDP+cIL9t0C/V6/bj6j5OcBjA87l/jeQ5LkuMZRfqmqvrWsHuq1wRQVS8A9zP6nsI0ruci4ONJ9jL6qZZbknyd6VwLAFX17PC4H7iN0U/vnNb17AP2Df/HBnAro3Af9nq6hXq9flz928Cnh+efZnSddyokCfBVYE9VfWnRS1O5piQzSU4dnr8NuAR4gilcT1VdU1WnV9Uso78r91bVJ5nCtQAkOTHJyQeeAx8FdjOl66mqHwHPJDlr2HUx8DhHsp61vuC+xAX4S4H/Av4b+NO1nucI5r8ZeA74P0b/Rb0S+DVG3/B5anh851rPeRjr+R1Gl58eBXYNX5dO65qA3wIeHtazG/jzYf9UrmfRuj7M699MnMq1MLqm+8jw9diBv//Tup5h9k3A/PDv2z8D7ziS9fgRcklqrtulD0nSIQy1JDVnqCWpOUMtSc0ZaklqzlBLUnOGWpKa+39oDot3Q2MtbQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most important features indeces: [26  3 36 35 37 32 30 31 34 33]\n",
      "0.08704236901200438\n",
      "2.7842877685031375\n",
      "139193286.7390346\n",
      "139193286.7394856\n",
      "519150263.33393466\n",
      "658343550.0728272\n",
      "658343550.0732841\n",
      "658343550.0726383\n",
      "658343550.0726708\n",
      "658343550.0726236\n"
     ]
    }
   ],
   "source": [
    "# write here the code for 2.4 Feature importance analysis\n",
    "# ref: https://machinelearningmastery.com/calculate-feature-importance-with-python/\n",
    "\n",
    "# linear regression feature importance\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "# define dataset\n",
    "X, y = df2numpy(df_model)\n",
    "\n",
    "# define the model\n",
    "model = LinearRegression()\n",
    "imp_lr = get_feature_importance(X,y, model)\n",
    "ind = np.argpartition(imp_lr, -10)[-10:]\n",
    "\n",
    "print('most important features indeces:', ind)\n",
    "for i in ind:\n",
    "    print(imp_lr[i])\n",
    "\n",
    "# model = LogisticRegression()\n",
    "# get_feature_importance(X,y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Normalize data with minmax scale</h2>\n",
    "after min max scale visualize dataframe\n",
    "<h2>df2numpy</h2>\n",
    "define a function to convert a dataframe to numpy array with X and y(label)\n",
    "<h2>Feature importance analysis</h2>\n",
    "with help of references that cite in code in comment, define a method to get feature importancy.\n",
    "define a function(get_feature_importance) that take X, y and model then fit model on X and y and then use model.coef as a impotancy feature and then print a summary about importancy of features and then plot and return it.\n",
    "then with numpy function argpartition get high 10 values indeces to get 10 most important features.\n",
    "we can use another ways to get important features like use LogisticRegression instead of LinearRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. Model Selection (up to 8.2 of 11.2  points)\n",
    "In this part of the challenge you are requested to perform all the necessary steps required in order to design a full fledged classification task on the <b>shares</b> feature.\n",
    "\n",
    "You are requested to perform the following steps having in mind the following: \n",
    "\n",
    "1) the dataset must be properly splitted to perform crossvalidation \n",
    "\n",
    "2) when required, features must be properly encoded\n",
    "\n",
    "3) in order to simplify the problem the target feature can be dicretized <b>(number of classes must be >=5)</b> ;\n",
    "\n",
    "4) for model selection you are requested to consider: \n",
    "\n",
    "- Decision Trees\n",
    "\n",
    "- Support Vector Machines;\n",
    "\n",
    "- An ensamble methodology;\n",
    "\n",
    "- MLPNs.\n",
    "\n",
    "5) hyper-parameter tuning <b>must</b> be performed and discussed;\n",
    "\n",
    "6) apply standardizion and normalization when appropriate;\n",
    "\n",
    "7) remember to use an appropriate evaluation setting (cross-fold etc..)\n",
    "\n",
    "8) describe the measures adopted for the evaluation and discuss the results;\n",
    "\n",
    "9) provide a discussion of the model selection, where you describe the differences in terms of performance and explains the root causes;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset for train from most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kw_avg_avg</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39629</th>\n",
       "      <td>0.063047</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39630</th>\n",
       "      <td>0.061186</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39631</th>\n",
       "      <td>0.069573</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39632</th>\n",
       "      <td>0.078307</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39633</th>\n",
       "      <td>0.040808</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39634 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       kw_avg_avg  n_unique_tokens  weekday_is_sunday  weekday_is_saturday  \\\n",
       "0        0.000000         0.000947                0.0                  0.0   \n",
       "1        0.000000         0.000863                0.0                  0.0   \n",
       "2        0.000000         0.000820                0.0                  0.0   \n",
       "3        0.000000         0.000719                0.0                  0.0   \n",
       "4        0.000000         0.000593                0.0                  0.0   \n",
       "...           ...              ...                ...                  ...   \n",
       "39629    0.063047         0.000607                0.0                  0.0   \n",
       "39630    0.061186         0.000932                0.0                  0.0   \n",
       "39631    0.069573         0.000755                0.0                  0.0   \n",
       "39632    0.078307         0.000993                0.0                  0.0   \n",
       "39633    0.040808         0.000770                0.0                  0.0   \n",
       "\n",
       "       is_weekend  weekday_is_wednesday  weekday_is_monday  \\\n",
       "0             0.0                   0.0                1.0   \n",
       "1             0.0                   0.0                1.0   \n",
       "2             0.0                   0.0                1.0   \n",
       "3             0.0                   0.0                1.0   \n",
       "4             0.0                   0.0                1.0   \n",
       "...           ...                   ...                ...   \n",
       "39629         0.0                   1.0                0.0   \n",
       "39630         0.0                   1.0                0.0   \n",
       "39631         0.0                   1.0                0.0   \n",
       "39632         0.0                   1.0                0.0   \n",
       "39633         0.0                   1.0                0.0   \n",
       "\n",
       "       weekday_is_tuesday  weekday_is_friday  weekday_is_thursday    shares  \n",
       "0                     0.0                0.0                  0.0  0.000702  \n",
       "1                     0.0                0.0                  0.0  0.000842  \n",
       "2                     0.0                0.0                  0.0  0.001778  \n",
       "3                     0.0                0.0                  0.0  0.001422  \n",
       "4                     0.0                0.0                  0.0  0.000598  \n",
       "...                   ...                ...                  ...       ...  \n",
       "39629                 0.0                0.0                  0.0  0.001659  \n",
       "39630                 0.0                0.0                  0.0  0.001422  \n",
       "39631                 0.0                0.0                  0.0  0.002133  \n",
       "39632                 0.0                0.0                  0.0  0.002252  \n",
       "39633                 0.0                0.0                  0.0  0.001303  \n",
       "\n",
       "[39634 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# write here the code for step 3 Model Selection\n",
    "# temp_ind = list(range(33, 38))\n",
    "temp_ind = list(ind)\n",
    "temp_ind.append(-1) # add label index\n",
    "df_most_imp = df_model.iloc[:, temp_ind]\n",
    "display(df_most_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df2numpy(df_most_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [2.]\n",
      " ...\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# Discretize the output into 5 classes\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "kbins = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "y = kbins.fit_transform(y.reshape(-1, 1))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check imbalacing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2.0: 8821, 4.0: 8078, 3.0: 8008, 0.0: 7925, 1.0: 6802})\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import counter to count each class occurance\n",
    "from collections import Counter\n",
    "y_count = Counter(y.reshape(-1))\n",
    "print(y_count)\n",
    "plt.bar(y_count.keys(), list(y_count.values()), align='center')\n",
    "plt.show()\n",
    "# Data is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devide data to train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.22 (+/- 0.01)\n",
      "[0.22306322 0.21935292 0.21549421 0.21567463 0.2229479 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Defining the model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# 5-fold Cross Validation\n",
    "scores = cross_val_score(model, X_train, y_train , cv=5)\n",
    "\n",
    "# mean Accuracy\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.26 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Defining the model\n",
    "model = SVC()\n",
    "\n",
    "# 5-fold Cross Validation\n",
    "scores = cross_val_score(model, X_train, y_train.reshape(-1), cv=5)\n",
    "\n",
    "# model mean Accuracy\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An ensamble methodology: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.24 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Defining the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# 5-fold Cross Validation\n",
    "scores = cross_val_score(model, X_train, y_train.reshape(-1), cv=5)\n",
    "\n",
    "# model mean Accuracy\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Hyper Parameter\n",
    "def get_hyper_res(model, X_train, y_train, X_test, y_test, parameters_sets):\n",
    "  scores = ['precision' , 'recall']\n",
    "  for score in scores:\n",
    "    print(score + ':\\n')\n",
    "    # select \"RandomizedSearchCV\" to perform on the models\n",
    "    clf = RandomizedSearchCV(model, parameters_sets, scoring='%s_macro' % score)\n",
    "    \n",
    "    # Fit the model with defined hyperparameters and training data\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Print the best parameters for the model (clf.best_params)\n",
    "    print(\"Best parameter set with \" + str(clf.best_score_) + ' ' + score + \" is:\")\n",
    "    print(clf.best_params_)\n",
    "    print('\\n')\n",
    "    print(\"Scores for parameter set is:\")\n",
    "    # Compute the model scores\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    # Pridicting the output of the model for test set\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    # Print the evaluation scores \n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameter Tuning with RandomizedSearchCV\n",
    "1. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'random_state': None,\n",
       " 'splitter': 'best'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All hyperparameters of Decision Tree\n",
    "DecisionTreeClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:\n",
      "\n",
      "Best parameter set with 0.26423284464778884 precision is:\n",
      "{'max_features': 9, 'max_depth': 7}\n",
      "\n",
      "\n",
      "Scores for parameter set is:\n",
      "0.253 (+/-0.026) for {'max_features': 4, 'max_depth': 7}\n",
      "0.257 (+/-0.018) for {'max_features': 8, 'max_depth': 7}\n",
      "0.261 (+/-0.014) for {'max_features': 4, 'max_depth': 9}\n",
      "0.258 (+/-0.017) for {'max_features': 6, 'max_depth': 7}\n",
      "0.264 (+/-0.011) for {'max_features': 9, 'max_depth': 7}\n",
      "0.260 (+/-0.022) for {'max_features': 7, 'max_depth': 7}\n",
      "0.255 (+/-0.014) for {'max_features': 2, 'max_depth': 13}\n",
      "0.261 (+/-0.002) for {'max_features': 8, 'max_depth': 11}\n",
      "0.253 (+/-0.006) for {'max_features': 5, 'max_depth': 7}\n",
      "0.253 (+/-0.010) for {'max_features': 8, 'max_depth': 13}\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.28      0.59      0.38      1186\n",
      "         1.0       0.19      0.01      0.03      1016\n",
      "         2.0       0.26      0.24      0.25      1330\n",
      "         3.0       0.26      0.13      0.17      1207\n",
      "         4.0       0.34      0.43      0.38      1207\n",
      "\n",
      "    accuracy                           0.29      5946\n",
      "   macro avg       0.27      0.28      0.24      5946\n",
      "weighted avg       0.27      0.29      0.25      5946\n",
      "\n",
      "\n",
      "recall:\n",
      "\n",
      "Best parameter set with 0.27570956897636145 recall is:\n",
      "{'max_features': 4, 'max_depth': 7}\n",
      "\n",
      "\n",
      "Scores for parameter set is:\n",
      "0.259 (+/-0.007) for {'max_features': 10, 'max_depth': 13}\n",
      "0.257 (+/-0.004) for {'max_features': 9, 'max_depth': 13}\n",
      "0.269 (+/-0.009) for {'max_features': 3, 'max_depth': 11}\n",
      "0.270 (+/-0.012) for {'max_features': 2, 'max_depth': 9}\n",
      "0.265 (+/-0.003) for {'max_features': 10, 'max_depth': 11}\n",
      "0.275 (+/-0.006) for {'max_features': 10, 'max_depth': 5}\n",
      "0.261 (+/-0.010) for {'max_features': 2, 'max_depth': 13}\n",
      "0.276 (+/-0.011) for {'max_features': 4, 'max_depth': 7}\n",
      "0.261 (+/-0.004) for {'max_features': 5, 'max_depth': 13}\n",
      "0.270 (+/-0.006) for {'max_features': 3, 'max_depth': 9}\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.29      0.48      0.36      1186\n",
      "         1.0       0.15      0.03      0.04      1016\n",
      "         2.0       0.23      0.27      0.25      1330\n",
      "         3.0       0.25      0.15      0.19      1207\n",
      "         4.0       0.34      0.42      0.37      1207\n",
      "\n",
      "    accuracy                           0.28      5946\n",
      "   macro avg       0.25      0.27      0.24      5946\n",
      "weighted avg       0.25      0.28      0.25      5946\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Defining the model\n",
    "tree_model= DecisionTreeClassifier()\n",
    "\n",
    "# Set the Hyperparameters for test\n",
    "parameters_sets = [{'max_depth' :range(5,15,2), 'max_features':range(2,11)}]\n",
    "# Call my function \n",
    "get_hyper_res(tree_model, X_train, y_train, X_test, y_test, parameters_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'break_ties': False,\n",
       " 'cache_size': 200,\n",
       " 'class_weight': None,\n",
       " 'coef0': 0.0,\n",
       " 'decision_function_shape': 'ovr',\n",
       " 'degree': 3,\n",
       " 'gamma': 'scale',\n",
       " 'kernel': 'rbf',\n",
       " 'max_iter': -1,\n",
       " 'probability': False,\n",
       " 'random_state': None,\n",
       " 'shrinking': True,\n",
       " 'tol': 0.001,\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the hyperparameters of SVM\n",
    "from sklearn.svm import SVC\n",
    "SVC().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:\n",
      "\n",
      "Best parameter set with 0.22409795741028304 precision is:\n",
      "{'kernel': 'rbf', 'gamma': 1, 'C': 1}\n",
      "\n",
      "\n",
      "Scores for parameter set is:\n",
      "0.044 (+/-0.000) for {'kernel': 'rbf', 'gamma': 0.001, 'C': 0.01}\n",
      "0.044 (+/-0.000) for {'kernel': 'rbf', 'gamma': 0.1, 'C': 0.01}\n",
      "0.130 (+/-0.059) for {'kernel': 'rbf', 'gamma': 1, 'C': 0.01}\n",
      "0.044 (+/-0.000) for {'kernel': 'rbf', 'gamma': 0.001, 'C': 1}\n",
      "0.212 (+/-0.010) for {'kernel': 'rbf', 'gamma': 0.1, 'C': 1}\n",
      "0.224 (+/-0.006) for {'kernel': 'rbf', 'gamma': 1, 'C': 1}\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.24      0.78      0.37      1186\n",
      "         1.0       0.00      0.00      0.00      1016\n",
      "         2.0       0.24      0.14      0.18      1330\n",
      "         3.0       0.29      0.09      0.14      1207\n",
      "         4.0       0.33      0.25      0.29      1207\n",
      "\n",
      "    accuracy                           0.26      5946\n",
      "   macro avg       0.22      0.25      0.19      5946\n",
      "weighted avg       0.23      0.26      0.20      5946\n",
      "\n",
      "recall:\n",
      "\n",
      "Best parameter set with 0.2512726482372263 recall is:\n",
      "{'kernel': 'rbf', 'gamma': 1, 'C': 1}\n",
      "\n",
      "\n",
      "Scores for parameter set is:\n",
      "0.200 (+/-0.000) for {'kernel': 'rbf', 'gamma': 0.001, 'C': 0.01}\n",
      "0.200 (+/-0.000) for {'kernel': 'rbf', 'gamma': 0.1, 'C': 0.01}\n",
      "0.227 (+/-0.005) for {'kernel': 'rbf', 'gamma': 1, 'C': 0.01}\n",
      "0.200 (+/-0.000) for {'kernel': 'rbf', 'gamma': 0.001, 'C': 1}\n",
      "0.235 (+/-0.007) for {'kernel': 'rbf', 'gamma': 0.1, 'C': 1}\n",
      "0.251 (+/-0.008) for {'kernel': 'rbf', 'gamma': 1, 'C': 1}\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.24      0.78      0.37      1186\n",
      "         1.0       0.00      0.00      0.00      1016\n",
      "         2.0       0.24      0.14      0.18      1330\n",
      "         3.0       0.29      0.09      0.14      1207\n",
      "         4.0       0.33      0.25      0.29      1207\n",
      "\n",
      "    accuracy                           0.26      5946\n",
      "   macro avg       0.22      0.25      0.19      5946\n",
      "weighted avg       0.23      0.26      0.20      5946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining the model\n",
    "svc_model = SVC()\n",
    "# Setting the Hyperpatameters\n",
    "parameters_sets = [{'kernel': ['rbf'], 'C': [0.01,1], 'gamma' : [0.001, 0.1,1]}]\n",
    "# Calling the defined get_hyper function\n",
    "get_hyper_res(svc_model, X_train, y_train, X_test, y_test, parameters_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting the hyperparameters of Random Forest\n",
    "RandomForestClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:\n",
      "\n",
      "Best parameter set with 0.2981461245472449 precision is:\n",
      "{'n_estimators': 46, 'max_features': 9, 'max_depth': 6}\n",
      "\n",
      "\n",
      "Scores for parameter set is:\n",
      "0.270 (+/-0.005) for {'n_estimators': 30, 'max_features': 8, 'max_depth': 9}\n",
      "0.254 (+/-0.008) for {'n_estimators': 26, 'max_features': 3, 'max_depth': 15}\n",
      "0.255 (+/-0.011) for {'n_estimators': 44, 'max_features': 8, 'max_depth': 14}\n",
      "0.298 (+/-0.086) for {'n_estimators': 46, 'max_features': 9, 'max_depth': 6}\n",
      "0.271 (+/-0.010) for {'n_estimators': 40, 'max_features': 8, 'max_depth': 9}\n",
      "0.257 (+/-0.013) for {'n_estimators': 38, 'max_features': 4, 'max_depth': 14}\n",
      "0.251 (+/-0.012) for {'n_estimators': 36, 'max_features': 6, 'max_depth': 17}\n",
      "0.237 (+/-0.006) for {'n_estimators': 24, 'max_features': 3, 'max_depth': 19}\n",
      "0.276 (+/-0.008) for {'n_estimators': 32, 'max_features': 7, 'max_depth': 8}\n",
      "0.279 (+/-0.017) for {'n_estimators': 26, 'max_features': 4, 'max_depth': 7}\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.29      0.53      0.37      1186\n",
      "         1.0       0.20      0.00      0.00      1016\n",
      "         2.0       0.25      0.31      0.28      1330\n",
      "         3.0       0.25      0.11      0.15      1207\n",
      "         4.0       0.34      0.45      0.39      1207\n",
      "\n",
      "    accuracy                           0.29      5946\n",
      "   macro avg       0.27      0.28      0.24      5946\n",
      "weighted avg       0.27      0.29      0.25      5946\n",
      "\n",
      "recall:\n",
      "\n",
      "Best parameter set with 0.2795277307022903 recall is:\n",
      "{'n_estimators': 36, 'max_features': 8, 'max_depth': 5}\n",
      "\n",
      "\n",
      "Scores for parameter set is:\n",
      "0.280 (+/-0.004) for {'n_estimators': 36, 'max_features': 8, 'max_depth': 5}\n",
      "0.257 (+/-0.007) for {'n_estimators': 32, 'max_features': 8, 'max_depth': 16}\n",
      "0.264 (+/-0.005) for {'n_estimators': 40, 'max_features': 7, 'max_depth': 14}\n",
      "0.277 (+/-0.009) for {'n_estimators': 58, 'max_features': 4, 'max_depth': 11}\n",
      "0.274 (+/-0.005) for {'n_estimators': 38, 'max_features': 8, 'max_depth': 12}\n",
      "0.278 (+/-0.003) for {'n_estimators': 22, 'max_features': 7, 'max_depth': 5}\n",
      "0.260 (+/-0.007) for {'n_estimators': 28, 'max_features': 9, 'max_depth': 15}\n",
      "0.278 (+/-0.007) for {'n_estimators': 20, 'max_features': 7, 'max_depth': 9}\n",
      "0.249 (+/-0.009) for {'n_estimators': 26, 'max_features': 5, 'max_depth': 18}\n",
      "0.267 (+/-0.002) for {'n_estimators': 18, 'max_features': 6, 'max_depth': 13}\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.29      0.54      0.37      1186\n",
      "         1.0       0.00      0.00      0.00      1016\n",
      "         2.0       0.26      0.28      0.27      1330\n",
      "         3.0       0.26      0.13      0.17      1207\n",
      "         4.0       0.33      0.45      0.38      1207\n",
      "\n",
      "    accuracy                           0.29      5946\n",
      "   macro avg       0.23      0.28      0.24      5946\n",
      "weighted avg       0.24      0.29      0.25      5946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Defining the model\n",
    "RandomForest_model = RandomForestClassifier()\n",
    "# Setting the hyperparameters to be tuned\n",
    "parameters_sets = [{'max_depth': range(5, 20), 'max_features':range(3,11),'n_estimators':range(10,60,2)}] \n",
    "# Calling the defined get_hyper function\n",
    "get_hyper_res(RandomForest_model, X_train, y_train, X_test, y_test, parameters_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Multi Layer Perceptron Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'batch_size': 'auto',\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 1e-08,\n",
       " 'hidden_layer_sizes': (100,),\n",
       " 'learning_rate': 'constant',\n",
       " 'learning_rate_init': 0.001,\n",
       " 'max_fun': 15000,\n",
       " 'max_iter': 200,\n",
       " 'momentum': 0.9,\n",
       " 'n_iter_no_change': 10,\n",
       " 'nesterovs_momentum': True,\n",
       " 'power_t': 0.5,\n",
       " 'random_state': None,\n",
       " 'shuffle': True,\n",
       " 'solver': 'adam',\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': False,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the hyperparameters of MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "MLPClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:\n",
      "\n",
      "Best parameter set with 0.23230942208994368 precision is:\n",
      "{'solver': 'adam', 'max_iter': 50, 'hidden_layer_sizes': 18}\n",
      "\n",
      "\n",
      "Scores for parameter set is:\n",
      "0.220 (+/-0.014) for {'solver': 'adam', 'max_iter': 10, 'hidden_layer_sizes': 18}\n",
      "0.225 (+/-0.007) for {'solver': 'adam', 'max_iter': 20, 'hidden_layer_sizes': 18}\n",
      "0.230 (+/-0.013) for {'solver': 'adam', 'max_iter': 30, 'hidden_layer_sizes': 18}\n",
      "0.230 (+/-0.008) for {'solver': 'adam', 'max_iter': 40, 'hidden_layer_sizes': 18}\n",
      "0.232 (+/-0.009) for {'solver': 'adam', 'max_iter': 50, 'hidden_layer_sizes': 18}\n",
      "0.219 (+/-0.012) for {'solver': 'adam', 'max_iter': 10, 'hidden_layer_sizes': 20}\n",
      "0.229 (+/-0.006) for {'solver': 'adam', 'max_iter': 20, 'hidden_layer_sizes': 20}\n",
      "0.230 (+/-0.008) for {'solver': 'adam', 'max_iter': 30, 'hidden_layer_sizes': 20}\n",
      "0.231 (+/-0.009) for {'solver': 'adam', 'max_iter': 40, 'hidden_layer_sizes': 20}\n",
      "0.231 (+/-0.012) for {'solver': 'adam', 'max_iter': 50, 'hidden_layer_sizes': 20}\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.27      0.66      0.38      1186\n",
      "         1.0       0.00      0.00      0.00      1016\n",
      "         2.0       0.25      0.21      0.23      1330\n",
      "         3.0       0.29      0.08      0.13      1207\n",
      "         4.0       0.33      0.43      0.37      1207\n",
      "\n",
      "    accuracy                           0.28      5946\n",
      "   macro avg       0.23      0.28      0.22      5946\n",
      "weighted avg       0.24      0.28      0.23      5946\n",
      "\n",
      "recall:\n",
      "\n",
      "Best parameter set with 0.27499749455891365 recall is:\n",
      "{'solver': 'adam', 'max_iter': 50, 'hidden_layer_sizes': 18}\n",
      "\n",
      "\n",
      "Scores for parameter set is:\n",
      "0.243 (+/-0.011) for {'solver': 'adam', 'max_iter': 10, 'hidden_layer_sizes': 18}\n",
      "0.264 (+/-0.015) for {'solver': 'adam', 'max_iter': 20, 'hidden_layer_sizes': 18}\n",
      "0.271 (+/-0.010) for {'solver': 'adam', 'max_iter': 30, 'hidden_layer_sizes': 18}\n",
      "0.273 (+/-0.007) for {'solver': 'adam', 'max_iter': 40, 'hidden_layer_sizes': 18}\n",
      "0.275 (+/-0.007) for {'solver': 'adam', 'max_iter': 50, 'hidden_layer_sizes': 18}\n",
      "0.246 (+/-0.009) for {'solver': 'adam', 'max_iter': 10, 'hidden_layer_sizes': 20}\n",
      "0.269 (+/-0.007) for {'solver': 'adam', 'max_iter': 20, 'hidden_layer_sizes': 20}\n",
      "0.270 (+/-0.007) for {'solver': 'adam', 'max_iter': 30, 'hidden_layer_sizes': 20}\n",
      "0.275 (+/-0.003) for {'solver': 'adam', 'max_iter': 40, 'hidden_layer_sizes': 20}\n",
      "0.273 (+/-0.007) for {'solver': 'adam', 'max_iter': 50, 'hidden_layer_sizes': 20}\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.26      0.68      0.38      1186\n",
      "         1.0       0.00      0.00      0.00      1016\n",
      "         2.0       0.26      0.19      0.22      1330\n",
      "         3.0       0.29      0.08      0.13      1207\n",
      "         4.0       0.33      0.43      0.37      1207\n",
      "\n",
      "    accuracy                           0.28      5946\n",
      "   macro avg       0.23      0.28      0.22      5946\n",
      "weighted avg       0.24      0.28      0.23      5946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining the model\n",
    "MLP_model = MLPClassifier()\n",
    "# Setting the hyperparameters to be tuned\n",
    "parameters_sets = [{'solver':['adam'],'hidden_layer_sizes':[(18),(20)],'max_iter':range(10,60,10)}]\n",
    "# Calling the pre-defined get_hyper function\n",
    "get_hyper_res(MLP_model, X_train, y_train, X_test, y_test, parameters_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model Selection</h2>\n",
    "2) when required, features must be properly encoded:\n",
    "all features are numbers then we dont need to encode them.\n",
    "<h3>Create dataset for train from most important features</h3>\n",
    "with indeces of most important feature from last cell and label (last column) create a data frame with good columns then convert this dataframe to numpy arrays.\n",
    "<h3>Discretization</h3>\n",
    "then we should Discretize label then we Discretize label with KBinsDiscretizer with 5 bins, then we have a classification problem with 5 class (0, 1, 2, 3, 4)\n",
    "<h3>Check imbalacing of data</h3>\n",
    "dataset is balance after Discretize labels to 5 class. (plot and number of each class show this.\n",
    "<h3>Devide data to train and test</h3>\n",
    "for a good modeling we need 3 devides of data: train, validation and test.\n",
    "test data is a group of data that models dont see them on train phase.\n",
    "validation data is a group of data that models dont see them for train but use them for parameter tuning, then best model after tuning is best model for validation data.\n",
    "and tarin data is data that model train with them.\n",
    "<h2>Train models</h2>\n",
    "train models on def values and then print accuracy and standard deviation as a error for our accuracy.\n",
    "<h3>Decision Tree</h3>\n",
    "first of all we train decision tree with cross validation with 5 folds.\n",
    "accuracy is 0.22\n",
    "<h3>SVM</h3>\n",
    "then we train svm with cross validation with 5 folds.\n",
    "accuracy is 0.26\n",
    "<h3>An ensamble methodology: Random Forest</h3>\n",
    "then we train Random Forest with cross validation with 5 folds.\n",
    "accuracy is 0.24\n",
    "<h2>Hyper Parameter Tuning</h2>\n",
    "after train example models with data, we go to fine tune parameters of models.\n",
    "we define a function that compute precision and recall for model with test data for result and print a complete report about model and scores.\n",
    "and we use RandomizedSearchCV for Hyper Parameter Tuning.\n",
    "<h3>1. Decision Tree</h3>\n",
    "first of all print all parameters we can play with them with .get_params() function.\n",
    "\n",
    "then select parameters that i think change them could help model to get better score and then give them examples for random testing on this parameters.\n",
    "for decision tree i test \"max_depth\" from 5 to 14 with stride 2 and \"max_features\" between range 2 and 10.\n",
    "\n",
    "Best parameter set with 0.26423284464778884 precision is:\n",
    "{'max_features': 9, 'max_depth': 7}\n",
    "\n",
    "Best parameter set with 0.27570956897636145 recall is:\n",
    "{'max_features': 4, 'max_depth': 7}\n",
    "<h3>2. SVM</h3>\n",
    "for SVM we use svc model and \"rbf kernel\" and c get values between 0.01 or 1 and gamma get value between 0.001 or 0.1 or 1.\n",
    "\n",
    "Best parameter set with 0.22409795741028304 precision is:\n",
    "{'kernel': 'rbf', 'gamma': 1, 'C': 1}\n",
    "\n",
    "Best parameter set with 0.2512726482372263 recall is:\n",
    "{'kernel': 'rbf', 'gamma': 1, 'C': 1}\n",
    "<h3>3. Random Forest</h3>\n",
    "for random forest we use \"max_depth\" between 5 to 19 and \"max_features\" between 3 and 10 and \"n_estimators\" between 10 and 59 with stride 2.\n",
    "\n",
    "Best parameter set with 0.2981461245472449 precision is:\n",
    "{'n_estimators': 46, 'max_features': 9, 'max_depth': 6}\n",
    "\n",
    "Best parameter set with 0.2795277307022903 recall is:\n",
    "{'n_estimators': 36, 'max_features': 8, 'max_depth': 5}\n",
    "<h3>4. MLPN</h3>\n",
    "for mlp we use 'adam' as solver and hidden layer size between 18 and 19 and max itteration between 10 to 50 with stride 10.\n",
    "\n",
    "Best parameter set with 0.23230942208994368 precision is:\n",
    "{'solver': 'adam', 'max_iter': 50, 'hidden_layer_sizes': 18}\n",
    "\n",
    "Best parameter set with 0.27499749455891365 recall is:\n",
    "{'solver': 'adam', 'max_iter': 50, 'hidden_layer_sizes': 18}\n",
    "<h2>Evaluation</h2>\n",
    "Between all of models, random forest has best score then we evaluation random forest with best parameters.\n",
    "Then random forest is best option for classification this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of Selected Model: RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.26      0.27      0.26      1186\n",
      "     class 1       0.17      0.16      0.17      1016\n",
      "      class2       0.23      0.23      0.23      1330\n",
      "      class3       0.22      0.21      0.21      1207\n",
      "      class4       0.30      0.30      0.30      1207\n",
      "\n",
      "    accuracy                           0.24      5946\n",
      "   macro avg       0.23      0.24      0.24      5946\n",
      "weighted avg       0.24      0.24      0.24      5946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Defining the model\n",
    "RandomForest_model = RandomForestClassifier()\n",
    "\n",
    "# fitting the model with training data\n",
    "RandomForest_model.fit(X_train,y_train)\n",
    "\n",
    "# Test Phase: Predict the output for unseen data \n",
    "y_pred = RandomForest_model.predict(X_test)\n",
    "\n",
    "# Compute the confusion matrix for the test data: It will be calculated based on the predicted output(y_pred) and the true output(y_test)\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# calculating the evaluation scores (precision, recall, f1-score, and support) for each class and the whole accuracy\n",
    "# Defining the target classes\n",
    "target_names = ['class 0', 'class 1','class2', 'class3','class4']\n",
    "\n",
    "# Calculating the evaluation scores \n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# 4. Summary\n",
    "Provide a summary discussion (in English) of your solution <b>(at least 500 words)</b> feel free to include plots figures and tables.\n",
    "\n",
    "<b>This is a mandatory step</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write here <b>your own</b> summary dicussion (in English) use at least 500 words,\n",
    "# Goal\n",
    "we have a dataset and we should classify this dataset with features that gives in this dataset and predict number of shares and for make problem easy than before, convert regression problem to classification problem with discretize labels to 5 bins.\n",
    "\n",
    "# Summary\n",
    "\n",
    "I loaded the csv file (dataset) as a pandas dataframe. To do this, I used pandas.read_csv() function. Then i cleaned dataset with multiple of if statement like remove NaN values and non numeric value in numeric columns and wrong format values in rows.\n",
    "for detail you can refer to cleaning dataset cells. and then save cleaned dataset and then another time load cleaned dataset in a dataframe.\n",
    "\n",
    "Then print and visualize some basic information about the dataset:\n",
    "1) Print the total number of samples\n",
    "In order to finding the total number of samples, it is enough to know the number of rows of the dataset. So, simply I got the shape of the dataset and printed the number of rows which is equal to 39634 after cleaning.\n",
    "\n",
    "2) Print a table with the first 15 samples\n",
    "To do this, I used the head() function. Since first 15 samples must be shown, head(15) is used to create a table with first 15 instances of our dataset.\n",
    "\n",
    "3) Plot the histogram distribution of \"shares\"\n",
    "For plotting the histogram, the hist() function of Matplotlib library is used.\n",
    "\n",
    "4) A bar chart counting the attributes: data_channel_is_lifestyle, data_channel_is_entertainment, data_channel_is_bus, data_channel_is_socmed, data_channel_is_tech, data_channel_is_world\n",
    "plot a bar chart for this columns (all of this columns are boolean columns, then we can plot them with sum of every column.\n",
    "\n",
    "<h3>Perform feature importance analysis</h3>\n",
    "Normalize data with minmax scale\n",
    "after min max scale visualize dataframe\n",
    "df2numpy: define a function to convert a dataframe to numpy array with X and y(label)\n",
    "Feature importance analysis:\n",
    "with help of references that cite in code in comment, define a method to get feature importancy. define a function(get_feature_importance) that take X, y and model then fit model on X and y and then use model.coef as a impotancy feature and then print a summary about importancy of features and then plot and return it. then with numpy function argpartition get high 10 values indeces to get 10 most important features. we can use another ways to get important features like use LogisticRegression instead of LinearRegression.\n",
    "\n",
    "After get important features we make a dataset with just important features and then we work with it.\n",
    "dataset has 61 columns and we use 10 of most important features.\n",
    "\n",
    "Now, we are going to the next step which is classification. Before, feeding data to the model it is required to perform some preprocessing methods to prepare dataset.\n",
    "\n",
    "when required, features must be properly encoded: all of out features are numbers and dont have any categorical values then we dont need to encode them.\n",
    "Create dataset for train from most important features\n",
    "with indeces of most important feature from last cell and label (last column) create a data frame with good columns then convert this dataframe to numpy arrays.\n",
    "\n",
    "after that we have to Discretization data:\n",
    "then we should Discretize label then we Discretize label with KBinsDiscretizer with 5 bins, then we have a classification problem with 5 class (0, 1, 2, 3, 4)\n",
    "\n",
    "Check imbalacing of data:\n",
    "dataset is balance after Discretize labels to 5 class. (plot and number of each class show this.\n",
    "\n",
    "Devide data to train and test:\n",
    "for a good modeling we need 3 devides of data: train, validation and test. test data is a group of data that models dont see them on train phase. validation data is a group of data that models dont see them for train but use them for parameter tuning, then best model after tuning is best model for validation data. and tarin data is data that model train with them.\n",
    "\n",
    "Train models\n",
    "train models on def values and then print accuracy and standard deviation as a error for our accuracy.\n",
    "\n",
    "Decision Tree\n",
    "first of all we train decision tree with cross validation with 5 folds. accuracy is 0.22\n",
    "\n",
    "SVM\n",
    "then we train svm with cross validation with 5 folds. accuracy is 0.26\n",
    "\n",
    "An ensamble methodology: Random Forest\n",
    "then we train Random Forest with cross validation with 5 folds. accuracy is 0.24\n",
    "\n",
    "Hyper Parameter Tuning\n",
    "after train example models with data, we go to fine tune parameters of models. we define a function that compute precision and recall for model with test data for result and print a complete report about model and scores. and we use RandomizedSearchCV for Hyper Parameter Tuning.\n",
    "\n",
    "As can be seen the accuracy of the classifires are low, so Hyperparameter tuning had applied to improve the accuracy of the models. It was based on two different scores: \"recall\" and \"Precision\"\n",
    "results for each of the models were as below:\n",
    "\n",
    "Decision Tree:\n",
    "Precision: max_features: 9, max_depth: 7, Acc: 26\n",
    "Recall: max_features: 4, max_depth: 7, Acc: 28\n",
    "\n",
    "Support Vector Machine:\n",
    "Precision: kernl: rbf, C: 1, gamma: 1 ,Acc: 22\n",
    "Recall: kernl: rbf, C: 1, gamma: 1 ,Acc: 25\n",
    "\n",
    "Random Forest:\n",
    "Precision: n_estimator: 46, max_features: 9, max_depth: 6, Acc: 30\n",
    "Recall: n_estimator: 36, max_features: 8, max_depth: 5, Acc: 28\n",
    "\n",
    "Multi Layer Perceptron:\n",
    "Precision: solver: adam, max_iter: 50, hidden_layer_sizes: 18, Acc: 23\n",
    "Recall: solver: adam, max_iter: 50, hidden_layer_sizes: 18, Acc: 27\n",
    "\n",
    "It is obvious that the accuracy of the classifiers have been improved after using hyperparameter tuning. With regards to selecting the best model since the accuracy of the RF is higher than others it was selected as the best model.\n",
    "\n",
    "Finally, Evaluation was examined. After selecting the best model, I went to the test phase. At this step, non-seen data (which was seprated before as X_test, y_test)used. First, I fit the model with the training data (X_train, y_train), then at the test phase the model was fed with unseen data (X_test) and the output of the model was compared with targets (ground truth). Finally, confusion matrix and the evaluation scores (with classification_report function) for the model were calculated.\n",
    "and model get 24 accuracy on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
